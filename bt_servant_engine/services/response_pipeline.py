"""Response processing pipeline: translation, combination, and chunking."""

from __future__ import annotations

import json
from typing import Any, List, Optional, cast

from langgraph.graph import END
from openai import OpenAI, OpenAIError
from openai.types.chat.chat_completion_message_param import ChatCompletionMessageParam
from openai.types.responses.easy_input_message_param import EasyInputMessageParam

from bt_servant_engine.core.agentic import ALLOWED_AGENTIC_STRENGTH
from bt_servant_engine.core.config import config
from bt_servant_engine.core.language import LANGUAGE_UNKNOWN
from bt_servant_engine.core.language import SUPPORTED_LANGUAGE_MAP as supported_language_map
from bt_servant_engine.core.logging import get_logger
from bt_servant_engine.services.preprocessing import detect_language as detect_language_impl
from bt_servant_engine.services.response_helpers import (
    normalize_single_response as normalize_single_response_impl,
    sample_for_language_detection as sample_for_language_detection_impl,
)
from utils import chop_text, combine_chunks
from utils.bible_locale import get_book_name
from utils.perf import add_tokens

logger = get_logger(__name__)

COMBINE_RESPONSES_SYSTEM_PROMPT = """
# Identity

You are a part of a RAG bot system that assists Bible translators. The decision system is a lang graph with various
nodes handling multiple user intents. Your job is to combine the response messages from various intent processing
nodes in the graph into one cohesive message that makes sense.

# Instructions

You will be given a json array of objects. Each object will have two properties: (1) the intent of the intent
processing node that generated the message. (2) the response message itself. In general, your job is to return a single
string representing the combined message. The combined message should be natural sounding, cohesive, and, to the degree
possible, contain all the elements of the individual messages. You will also be given the conversation history and the
user's most recent message. Leverage this context when combining response messages! Below are six guidelines for you to
use when combining messages:

(1) if the first-interaction intent was processed, the information and message related to this intent SHOULD ALWAYS
COME FIRST!!!

(2) If the CONVERSE_WITH_BT_SERVANT intent was processed, the combined message should usually start with some version of
the response message generated by this intent processing node. The only thing that should ever go before this is the
information and message related to the "first-interaction" intent.

(3) If the SET_RESPONSE_LANGUAGE intent was processed, the combined message should usually end with some version of
the response message generated by this intent processing node.

(4) If the GET_BIBLE_TRANSLATION_ASSISTANCE intent, or the GET_PASSAGE_SUMMARY intent, was processed, the information
contained in the response message generated by these intent processing nodes should usually be as close to the
beginning as possible, unless that would violate guideline #1 above.

(5) If some combination of the PERFORM_UNSUPPORTED_FUNCTION and RETRIEVE_SYSTEM_INFORMATION intents were processed, the
information from the associated response messages should usually fall in the middle somewhere.

(6) Make sure to synthesize/remove any repeated or redundant information. This is very important!!!

(7) If there are multiple questions found in the various responses, these must be reduced to one question, and that
question must be at the end of the message. Any question in the combined response must come at the very end of the
message.

(8) If you detect in conversation history that you've already said hello, there's no need to say it again.

(9) If it doesn't make sense to say "hello!" to the user, based on their most recent message, there's no need to say
'Hello!  I'm here to assist with Bible translation tasks' again.

(10) Remove duplicated boilerplate or repeated feature lists if multiple nodes include similar guidance; keep only one
concise version where appropriate.

Don't worry about the combined response being too big. A downstream node will chunk the message if needed.
"""

RESPONSE_TRANSLATOR_SYSTEM_PROMPT = """
    You are a translator for the final output in a chatbot system. You will receive text that
    needs to be translated into the language represented by the specified ISO 639-1 code.
"""

CHOP_AGENT_SYSTEM_PROMPT = (
    "You are an agent tasked to ensure that a message intended for Whatsapp fits within the 1500 character limit. Chop "
    "the supplied text in the biggest possible semantic chunks, while making sure no chuck is >= 1500 characters. "
    "Your output should be a valid JSON array containing strings (wrapped in double quotes!!) constituting the chunks. "
    "Only return the json array!! No ```json wrapper or the like. Again, make chunks as big as possible!!!"
)


def reconstruct_structured_text(resp_item: dict | str, localize_to: Optional[str]) -> str:
    """Render a response item to plain text, optionally localizing the header book name.

    - If `resp_item` is a plain string, return it.
    - If structured with segments, rebuild: "<Book> <suffix>:\n\n<scripture>".
    - If `localize_to` is provided, map the book to that language via get_book_name; else use canonical.
    """
    if isinstance(resp_item, str):
        return resp_item
    body = cast(dict | str, resp_item.get("response"))
    if isinstance(body, dict) and isinstance(body.get("segments"), list):
        segs = cast(list, body.get("segments"))
        header_book = ""
        header_suffix = ""
        scripture_text = ""
        for seg in segs:
            if not isinstance(seg, dict):
                continue
            st = seg.get("type")
            txt = cast(str, seg.get("text", ""))
            if st == "header_book":
                header_book = txt
            elif st == "header_suffix":
                header_suffix = txt
            elif st == "scripture":
                scripture_text = txt
        book = get_book_name(localize_to or "en", header_book) if localize_to else header_book
        header = (f"{book} {header_suffix}" if header_suffix else book).strip() + ":"
        return header + ("\n\n" + scripture_text if scripture_text else "")
    return str(body)


def combine_responses(
    client: OpenAI,
    chat_history: list[dict[str, str]],
    latest_user_message: str,
    responses: list[dict[str, Any]],
    extract_cached_input_tokens_fn: callable,
) -> str:
    """Ask OpenAI to synthesize multiple node responses into one coherent text.

    Note: callers should pass only normal responses here; scripture-protected
    items are excluded earlier to avoid rewriting scripture text.
    """
    uncombined_responses = json.dumps(responses)
    logger.info("preparing to combine responses:\n\n%s", uncombined_responses)
    messages: list[EasyInputMessageParam] = [
        {
            "role": "developer",
            "content": f"conversation history: {chat_history}",
        },
        {
            "role": "developer",
            "content": f"latest user message: {latest_user_message}",
        },
        {
            "role": "developer",
            "content": f"responses to synthesize: {uncombined_responses}",
        },
    ]
    response = client.responses.create(
        model="gpt-4o",
        instructions=COMBINE_RESPONSES_SYSTEM_PROMPT,
        input=cast(Any, messages),
    )
    usage = getattr(response, "usage", None)
    if usage is not None:
        it = getattr(usage, "input_tokens", None)
        ot = getattr(usage, "output_tokens", None)
        tt = getattr(usage, "total_tokens", None)
        if tt is None and (it is not None or ot is not None):
            tt = (it or 0) + (ot or 0)
        cit = extract_cached_input_tokens_fn(usage)
        add_tokens(it, ot, tt, model="gpt-4o", cached_input_tokens=cit)
    combined = response.output_text
    logger.info("combined response from openai: %s", combined)
    return combined


def translate_text(
    client: OpenAI,
    response_text: str,
    target_language: str,
    model_for_agentic_strength_fn: callable,
    extract_cached_input_tokens_fn: callable,
    *,
    agentic_strength: Optional[str] = None,
) -> str:
    """Translate a single text into the target ISO 639-1 language code.

    Returns a plain string. If the OpenAI SDK returns a structured content
    list or None, normalize it to a string.
    """
    resolved_strength = agentic_strength if agentic_strength in ALLOWED_AGENTIC_STRENGTH else None
    if resolved_strength is None:
        configured = getattr(config, "AGENTIC_STRENGTH", "normal")
        resolved_strength = configured if configured in ALLOWED_AGENTIC_STRENGTH else "normal"
    model_name = model_for_agentic_strength_fn(
        resolved_strength, allow_low=False, allow_very_low=True
    )
    chat_messages = cast(
        List[ChatCompletionMessageParam],
        [
            {
                "role": "system",
                "content": RESPONSE_TRANSLATOR_SYSTEM_PROMPT,
            },
            {
                "role": "user",
                "content": (
                    f"text to translate: {response_text}\n\n"
                    f"ISO 639-1 code representing target language: {target_language}"
                ),
            },
        ],
    )
    completion = client.chat.completions.create(
        model=model_name,
        messages=chat_messages,
    )
    usage = getattr(completion, "usage", None)
    if usage is not None:
        it = getattr(usage, "prompt_tokens", None)
        ot = getattr(usage, "completion_tokens", None)
        tt = getattr(usage, "total_tokens", None)
        cit = extract_cached_input_tokens_fn(usage)
        add_tokens(it, ot, tt, model=model_name, cached_input_tokens=cit)
    content = completion.choices[0].message.content
    if isinstance(content, list):
        text = "".join(part.get("text", "") if isinstance(part, dict) else "" for part in content)
    elif content is None:
        text = ""
    else:
        text = content
    logger.info("chunk: \n%s\n\ntranslated to:\n%s", response_text, text)
    return cast(str, text)


def translate_or_localize_response(
    client: OpenAI,
    resp: dict | str,
    target_language: str,
    agentic_strength: str,
    model_for_agentic_strength_fn: callable,
    extract_cached_input_tokens_fn: callable,
) -> str:
    """Translate free-form text or localize structured scripture outputs."""
    if isinstance(resp, str):
        sample = sample_for_language_detection_impl(resp)
        detected_lang = (
            detect_language_impl(client, sample, agentic_strength=agentic_strength)
            if sample
            else target_language
        )
        if detected_lang != target_language:
            logger.info("preparing to translate to %s", target_language)
            return translate_text(
                client,
                response_text=resp,
                target_language=target_language,
                model_for_agentic_strength_fn=model_for_agentic_strength_fn,
                extract_cached_input_tokens_fn=extract_cached_input_tokens_fn,
                agentic_strength=agentic_strength,
            )
        logger.info("chunk translation not required. using chunk as is.")
        return resp

    body = cast(dict | str, resp.get("response"))
    if isinstance(body, dict) and isinstance(body.get("segments"), list):
        item_lang = cast(Optional[str], body.get("content_language"))
        header_is_translated = bool(body.get("header_is_translated"))
        localize_to = None if header_is_translated else (item_lang or target_language)
        return reconstruct_structured_text(resp_item=resp, localize_to=localize_to)
    return str(body)


def build_translation_queue(
    state: dict[str, Any],
    protected_items: list[dict],
    normal_items: list[dict],
    combine_responses_fn: callable,
) -> list[dict | str]:
    """Assemble responses in the order they should be translated or localized."""
    queue: list[dict | str] = list(protected_items)
    non_combinable: list[dict] = [i for i in normal_items if i.get("suppress_combining")]
    combinable: list[dict] = [i for i in normal_items if not i.get("suppress_combining")]

    for item in non_combinable:
        queue.append(normalize_single_response_impl(item))

    if not combinable:
        return queue
    if len(combinable) == 1:
        queue.append(normalize_single_response_impl(combinable[0]))
        return queue
    queue.append(
        combine_responses_fn(
            state["user_chat_history"],
            state["user_query"],
            combinable,
        )
    )
    return queue


def resolve_target_language(
    state: dict[str, Any],
    responses_for_translation: list[dict | str],
) -> tuple[Optional[str], Optional[list[str]]]:
    """Determine the target language or build a pass-through fallback."""
    user_language = cast(Optional[str], state.get("user_response_language"))
    if user_language:
        return user_language, None

    target_language = cast(str, state.get("query_language"))
    if target_language != LANGUAGE_UNKNOWN:
        return target_language, None

    logger.warning("target language unknown. bailing out.")
    passthrough_texts: list[str] = [
        reconstruct_structured_text(resp_item=resp, localize_to=None)
        for resp in responses_for_translation
    ]
    notice = (
        "You haven't set your desired response language and I wasn't able to determine the language of your "
        "original message in order to match it. You can set your desired response language at any time by "
        "saying: Set my response language to Spanish, or Indonesian, or any of the supported languages: "
        f"{', '.join(supported_language_map.keys())}."
    )
    passthrough_texts.append(notice)
    return None, passthrough_texts


def chunk_message(
    client: OpenAI,
    text_to_chunk: str,
    extract_cached_input_tokens_fn: callable,
    additional_responses: list[str],
    chunk_max: int,
) -> list[str]:
    """Chunk oversized responses to respect WhatsApp limits, via LLM or fallback."""
    logger.info("MESSAGE TOO BIG. CHUNKING...")
    try:
        chat_messages = cast(
            List[ChatCompletionMessageParam],
            [
                {
                    "role": "system",
                    "content": CHOP_AGENT_SYSTEM_PROMPT,
                },
                {
                    "role": "user",
                    "content": f"text to chop: \n\n{text_to_chunk}",
                },
            ],
        )
        completion = client.chat.completions.create(
            model="gpt-4o",
            messages=chat_messages,
        )
        usage = getattr(completion, "usage", None)
        if usage is not None:
            it = getattr(usage, "prompt_tokens", None)
            ot = getattr(usage, "completion_tokens", None)
            tt = getattr(usage, "total_tokens", None)
            cit = extract_cached_input_tokens_fn(usage)
            add_tokens(it, ot, tt, model="gpt-4o", cached_input_tokens=cit)
        response_content = completion.choices[0].message.content
        if not isinstance(response_content, str):
            raise ValueError("empty or non-text content from chat completion")
        chunks = json.loads(response_content)
    except (OpenAIError, json.JSONDecodeError, ValueError):
        logger.error("LLM chunking failed. Falling back to deterministic chunking.", exc_info=True)
        chunks = None

    # Deterministic safeguards: if LLM returned a single massive chunk or invalid shape,
    # or if we skipped to fallback
    def _pack_items(items: list[str], max_len: int) -> list[str]:
        out: list[str] = []
        cur = ""
        for it in items:
            sep = ", " if cur else ""
            if len(cur) + len(sep) + len(it) <= max_len:
                cur += sep + it
            else:
                if cur:
                    out.append(cur)
                if len(it) <= max_len:
                    cur = it
                else:
                    # hard-split this long token
                    for j in range(0, len(it), max_len):
                        out.append(it[j : j + max_len])
                    cur = ""
        if cur:
            out.append(cur)
        return out

    if not isinstance(chunks, list) or any(not isinstance(c, str) for c in chunks):
        # Try delimiter-aware fallback for comma-heavy lists first
        if text_to_chunk.count(",") >= 10:
            parts = [p.strip() for p in text_to_chunk.split(",") if p.strip()]
            chunks = _pack_items(parts, chunk_max)
        else:
            chunks = chop_text(text=text_to_chunk, n=chunk_max)
    else:
        # Ensure each chunk respects the limit; if not, re-split deterministically
        fixed: list[str] = []
        for c in chunks:
            if len(c) <= chunk_max:
                fixed.append(c)
            else:
                if c.count(",") >= 10:
                    parts = [p.strip() for p in c.split(",") if p.strip()]
                    fixed.extend(_pack_items(parts, chunk_max))
                else:
                    fixed.extend(chop_text(text=c, n=chunk_max))
        chunks = fixed

    chunks.extend(additional_responses)
    return combine_chunks(chunks=chunks, chunk_max=chunk_max)


def needs_chunking(state: dict[str, Any]) -> str:
    """Return next node key if chunking is required, otherwise finish."""
    responses = state.get("translated_responses", [])
    if not responses:
        logger.info("[chunk-check] no text responses to send; skipping chunking")
        return END
    first_response = responses[0]
    if len(first_response) > config.MAX_META_TEXT_LENGTH:
        logger.warning("message to big: %d chars. preparing to chunk.", len(first_response))
        return "chunk_message_node"
    return END


__all__ = [
    "COMBINE_RESPONSES_SYSTEM_PROMPT",
    "RESPONSE_TRANSLATOR_SYSTEM_PROMPT",
    "CHOP_AGENT_SYSTEM_PROMPT",
    "reconstruct_structured_text",
    "combine_responses",
    "translate_text",
    "translate_or_localize_response",
    "build_translation_queue",
    "resolve_target_language",
    "chunk_message",
    "needs_chunking",
]
