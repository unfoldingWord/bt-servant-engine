"""Decision graph and message-processing pipeline for BT Servant.

This module defines the state, nodes, and orchestration logic for handling
incoming user messages, classifying intents, querying resources, and producing
final responses (including translation and chunking when necessary).
"""
# pylint: disable=line-too-long,too-many-lines

from __future__ import annotations

import json
import operator
from pathlib import Path
from typing import Annotated, Dict, List, TypedDict, cast, Any
from enum import Enum

from openai import OpenAI, OpenAIError
from openai.types.responses.easy_input_message_param import EasyInputMessageParam
from openai.types.chat.chat_completion_message_param import ChatCompletionMessageParam
from langgraph.graph import END, StateGraph
from pydantic import BaseModel

from logger import get_logger
from config import config
from utils import chop_text, combine_chunks
from db import (
    get_chroma_collection,
    is_first_interaction,
    set_first_interaction,
    set_user_response_language,
)

FEATURES_SUMMARY_RESPONSE = (
    "Currently, I can do three main things: summarize a passage, provide key words in a "
    "passage, or provide the typical translation challenges found in a passage. Here are some "
    "example questions or commands corresponding to these three functions: "
)
BOILER_PLATE_AVAILABLE_FEATURES_MESSAGE = f"""
    {FEATURES_SUMMARY_RESPONSE}

    (1) Please summarize Titus chapter 1.
    (2) List all the important words in Romans 1.
    (3) What challenges might I face when translating John 1:1?

    Which of these would you like me to do?
"""

FIRST_INTERACTION_MESSAGE = f"""
Hello! I am the BT Servant. This is our first conversation. Let's work together to understand and translate God's word!

{BOILER_PLATE_AVAILABLE_FEATURES_MESSAGE}
"""

UNSUPPORTED_FUNCTION_AGENT_SYSTEM_PROMPT = f"""
# Identity

You are a part of a RAG bot system that assists Bible translators. You are one node in the decision/intent processing 
lang graph. Specifically, your job is to handle the perform-unsupported-function intent. This means the user is trying 
to perform an unsupported function.

# Instructions

Respond appropriately to the user's request to do something that you currently can't do. Leverage the 
user's message and the conversation history if needed. Make sure to always end your response with some version of  
the boiler plate available features message (see below).

<boiler_plate_available_features_message>
    {BOILER_PLATE_AVAILABLE_FEATURES_MESSAGE}
</boiler_plate_available_features_message>
"""

COMBINE_RESPONSES_SYSTEM_PROMPT = """
# Identity

You are a part of a RAG bot system that assists Bible translators. The decision system is a lang graph with various 
nodes handling multiple user intents. Your job is to combine the response messages from various intent processing 
nodes in the graph into one cohesive message that makes sense.

# Instructions

You will be given a json array of objects. Each object will have two properties: (1) the intent of the intent 
processing node that generated the message. (2) the response message itself. In general, your job is to return a single 
string representing the combined message. The combined message should be natural sounding, cohesive, and, to the degree 
possible, contain all the elements of the individual messages. You will also be given the conversation history and the 
user's most recent message. Leverage this context when combining response messages! Below are six guidelines for you to 
use when combining messages:

(1) if the first-interaction intent was processed, the information and message related to this intent SHOULD ALWAYS 
COME FIRST!!!

(2) If the CONVERSE_WITH_BT_SERVANT intent was processed, the combined message should usually start with some version of
the response message generated by this intent processing node. The only thing that should ever go before this is the 
information and message related to the "first-interaction" intent.

(3) If the SET_RESPONSE_LANGUAGE intent was processed, the combined message should usually end with some version of 
the response message generated by this intent processing node.

(4) If the GET_BIBLE_TRANSLATION_ASSISTANCE intent was processed, the information contained in the response message 
generated by this intent processing node should usually be as close to the beginning as possible, unless that would 
violate guideline #1 above. 

(5) If some combination of the PERFORM_UNSUPPORTED_FUNCTION and RETRIEVE_SYSTEM_INFORMATION intents were processed, the 
information from the associated response messages should usually fall in the middle somewhere. 

(6) Make sure to synthesize/remove any repeated or redundant information. This is very important!!!

(7) If there are multiple questions found in the various responses, these must be reduced to one question, and that 
question must be at the end of the message. Any question in the combined response must come at the very end of the 
message.

(8) If you detect in conversation history that you've already said hello, there's no need to say it again.

(9) If it doesn't make sense to say "hello!" to the user, based on their most recent message, there's no need to say 
'Hello!  I'm here to assist with Bible translation tasks' again.

Don't worry about the combined response being too big. A downstream node will chunk the message if needed.
"""

CONVERSE_AGENT_SYSTEM_PROMPT = f"""
# Identity

You are a part of a RAG bot system that assists Bible translators. You are one node in the decision/intent processing 
lang graph. Specifically, your job is to handle the converse-with-bt-servant intent by responding conversationally to 
the user based on the provided context.

# Instructions

If we are here in the decision graph, the converse-with-bt-servant intent has been detected. You will be provided with 
the user's most recent message and conversation history. Your job is to respond conversationally to the user. Unless it 
doesn't make sense to do so, aim to end your response with some version of  the boiler plate available features message 
(see below).

<boiler_plate_available_features_message>
    {BOILER_PLATE_AVAILABLE_FEATURES_MESSAGE}
</boiler_plate_available_features_message>
"""

HELP_AGENT_SYSTEM_PROMPT = """
# Identity

You are a part of a WhatsApp RAG bot system that assists Bible translators called BT Servant. You sole purpose is to 
provide help information about the BT Servant system. If this node has been hit, it means the system has already 
classified the user's most recent message as a desire to receive help or more information about the system. This is 
typically the result of them saying something like: 'help!' or 'tell me about yourself' or 'how does this work?' Thus, 
make sure to always provide some help, to the best of your abilities. Always provide help to the user.

# Instructions
You will be supplied with the user's most recent message and also past conversation history. Using this context, 
provide the user with information detailing how the system works (the features of the BT Servant system). When doing 
so, leverage the feature information below. In almost all situations, when appropriate, end your response with a 
question asking the user if they want to use one of the first three features (example: 'Would you like me to summarize 
a verse or passage for you?').

# Features

1. Summarize books or passages of Scripture. (Example usage: 'please summarize Titus 2'; Please summarize Mark 1:1-8.)

2. Provide translation issues found in a verse range, chapter, or book. (Example usage: 'Tell me all the translation 
challenges in Mark 1:2.')

3. Provide a list of keywords found in a verse range, chapter, or book. (Example usage: 'What are all the keywords in 
Titus 1:5-16?')

4. Set the response language for the user. This is a persistent setting for the user and determines the response 
language of the system. If this is not set, the system tries to respond using the same language as the user's message. 
Currently, the supported languages are: English, Arabic, French, Spanish, Hindi, Russian, Indonesian, Swahili, 
Portuguese, Mandarin, and Dutch. (Example usage: Please set my response language to Spanish.)

# Using prior history for better responses

Here are some guidelines for using history for better responses:
1. If you detect in conversation history that you've already said hello, there's no need to say it again.
2. If it doesn't make sense to say "hello!" to the user, based on their most recent message, there's no need to say 
'Hello!  I'm here to assist with Bible translation tasks' again.
"""


RESPONSE_TRANSLATOR_SYSTEM_PROMPT = """
    You are a translator for the final output in a chatbot system. You will receive text that 
    needs to be translated into the language represented by the specified ISO 639-1 code.
"""


PREPROCESSOR_AGENT_SYSTEM_PROMPT = """
# Identity

You are a preprocessor agent/node in a retrieval augmented generation (RAG) pipeline. 

# Instructions

Use past conversation context, 
if supplied and applicable, to disambiguate or clarify the intent or meaning of the user's current message. Change 
as little as possible. Change nothing unless necessary. If the intent of the user's message is already clear, 
change nothing. Never greatly expand the user's current message. Changes should be small or none. Feel free to fix 
obvious spelling mistakes or errors, but not logic errors like incorrect books of the Bible. Return the clarified 
message and the reasons for clarifying or reasons for not changing anything. Examples below.

# Examples

## Example 1

<past_conversation>
    user_message: Summarize the book of Titus.
    assistant_response: The book of titus is about...
</past_conversation>

<current_message>
    user_message: Now Mark
</current_message>

<assistant_response>
    new_message: Now Summarize the book of Mark.
    reason_for_decision: Based on previous context, the user wants the system to do the same thing, but this time 
                         with Mark.
    message_changed: True
</assistant_response>
    
## Example 2

<past_conversation>
    user_message: What is going on in 1 Peter 3:7?
    assistant_response: Peter is instructing Christian husbands to be loving to their wives.
</past_conversation>

<current_message>
    user_message: Summarize Mark 3:1
</current_message>
    
<assistant_response>
    new_message: Summarize Mark 3:1.
    reason_for_decision: Nothing was changed. The user's current command has nothing to do with past context and
                         is fine as is.
    message_changed: False
</assistant_response>

## Example 3

<past_conversation>
    user_message: Explain John 1:1
    assistant_response: John claims that Jesus, the Word, existed in the beginning with God the Father.
</past_conversation>

<current_message>
    user_message: Explain John 1:3
</current_message>
    
<assistant_response>
    new_message: Explain John 1:3.
    reason_for_decision: The word 'John' was misspelled in the message.
    message_changed: True
</assistant_response>
"""


FINAL_RESPONSE_AGENT_SYSTEM_PROMPT = """
You are an assistant to Bible translators. Your main job is to answer questions about content found in various biblical 
resources: commentaries, translation notes, bible dictionaries, and various resources like FIA. In addition to answering
questions, you may be called upon to: summarize the data from resources, transform the data from resources (like
explaining it a 5-year old level, etc, and interact with the resources in all kinds of ways. All this is a part of your 
responsibilities. Context from resources (RAG results) will be provided to help you answer the question(s). Only answer 
questions using the provided context from resources!!! If you can't confidently figure it out using that context, 
simply say 'Sorry, I couldn't find any information in my resources to service your request or command. But 
maybe I'm unclear on your intent. Could you perhaps state it a different way?' You will also be given the past 
conversation history. Use this to understand the user's current message or query if necessary. If the past conversation 
history is not relevant to the user's current message, just ignore it. FINALLY, UNDER NO CIRCUMSTANCES ARE YOU TO SAY 
ANYTHING THAT WOULD BE DEEMED EVEN REMOTELY HERETICAL BY ORTHODOX CHRISTIANS. If you can't do what the user is asking 
because your response would be heretical, explain to the user why you cannot comply with their request or command.
"""

CHOP_AGENT_SYSTEM_PROMPT = (
    "You are an agent tasked to ensure that a message intended for Whatsapp fits within the 1500 character limit. Chop "
    "the supplied text in the biggest possible semantic chunks, while making sure no chuck is >= 1500 characters. "
    "Your output should be a valid JSON array containing strings (wrapped in double quotes!!) constituting the chunks. "
    "Only return the json array!! No ```json wrapper or the like. Again, make chunks as big as possible!!!"
)

INTENT_CLASSIFICATION_AGENT_SYSTEM_PROMPT = """
You are a node in a chatbot system called “BT Servant”, which provides intelligent assistance to Bible translators. Your 
job is to classify the **intent(s)** of the user’s latest message. Always return **at least one** intent from the 
approved list. However, if more than one intent is found, make sure to return those as well. If you're unsure, return 
`perform-unsupported-function`. If the user is asking for something outside the scope of the Bible, Bible translation, 
the Bible translation process, or one of the resources stored in the system (ex. Translation Notes, FIA resources, 
the Bible, Translation Words, Greek or Hebrew resources, commentaries, Bible dictionaries, etc.), or something outside 
system capabilities (defined by the various intents), also return the `perform-unsupported-function` intent.

You MUST always return at least one intent. You MUST choose one or more intents from the following five intent types:

<intents>
  <intent name="get-bible-translation-assistance">
    The user is asking for help with Bible translation — including understanding meaning; finding source verses; 
    clarifying language issues; consulting translation resources (ex. Translation Notes, FIA, the Bible, etc); receiving
    explanation of resources; interacting with resource content; asking for transformations of resource content 
    (ex. summaries of resource portions, biblical content, etc); or how to handle specific words, phrases, 
    or translation challenges.
  </intent>
  <intent name="set-response-language">
    The user wants to change the language in which the system responds. They might ask for responses in 
    Spanish, French, Arabic, etc.
  </intent>
  <intent name="retrieve-system-information">
    The user wants information about the BT Servant system itself — how it works, where it gets data, uptime, 
    example questions, supported languages, features, or current system configuration (like the documents currently 
    stored in the ChromaDB (vector database).
  </intent>
  <intent name="perform-unsupported-function">
    The user is asking BT Servant to do something outside the scope of Bible translation help, interacting with the 
    resources in the vector database, or system diagnostics. For example, telling jokes, setting timers, 
    summarizing current news, or anything else COMPLETELY UNRELATED to what BT Servant can do.
  </intent>
  <intent name="converse-with-bt-servant">
    The user is trying to talk to bt-servant (the bot/system). This represents any attempt to engage in conversation, 
    including simple greetings like: hello, hi, or even what's up! It also includes random conversation or statements 
    from the user. Essentially, this intent should be used if none of the other intent classifications make sense.
  </intent>
</intents>

Here are a few examples to guide you:

<examples>
  <example>
    <message>What is the best way to translate the word 'faith' in this passage?</message>
    <intent>get-bible-translation-assistance</intent>
  </example>
  <example>
    <message>What is the fourth step of the FIA process?</message>
    <intent>get-bible-translation-assistance</intent>
  </example>
  <example>
    <message>Explain the FIA process to me like I'm a three year old.</message>
    <intent>get-bible-translation-assistance</intent>
  </example>
  <example>
    <message>What is a FIA process in Mark.</message>
    <intent>get-bible-translation-assistance</intent>
  </example>
  <example>
    <message>Summarize Mark 3.</message>
    <intent>get-bible-translation-assistance</intent>
  </example>
  <example>
    <message>Summarize Titus 3:4</message>
    <intent>get-bible-translation-assistance</intent>
  </example>
  <example>
    <message>Can you reply to me in French from now on?</message>
    <intent>set-response-language</intent>
  </example>
  <example>
    <message>Where does BT Servant get its information from?</message>
    <intent>retrieve-system-information</intent>
  </example>
  <example>
    <message>Help</message>
    <intent>retrieve-system-information</intent>
  </example>
  <example>
    <message>Can you tell me a joke?</message>
    <intent>perform-unsupported-function</intent>
  </example>
  <example>
    <message>Hmm, what was I saying again?</message>
    <intent>converse-with-bt-servant</intent>
  </example>
  <example>
    <message>hello</message>
    <intent>converse-with-bt-servant</intent>
  </example>
  <example>
    <message>hi</message>
    <intent>converse-with-bt-servant</intent>
  </example>
  <example>
    <message>what's up!</message>
    <intent>converse-with-bt-servant</intent>
  </example>
  <example>
    <message>Good morning</message>
    <intent>converse-with-bt-servant</intent>
  </example>
  <example>
    <message>How are you doing today?</message>
    <intent>converse-with-bt-servant</intent>
  </example>
</examples>

You will return a single structured output like this:
```json
{ "intents": ["get-bible-translation-assistance"] }
```
"""

DETECT_LANGUAGE_AGENT_SYSTEM_PROMPT = """
    Your job is simply to detect the language of the supplied text. Attempt to match it to one of
    the 10 supported languages. If you can't, match it to OTHER.
"""

BASE_DIR = Path(__file__).resolve().parent
DB_DIR = config.DATA_DIR

open_ai_client = OpenAI(api_key=config.OPENAI_API_KEY)

supported_language_map = {
    "en": "English",
    "ar": "Arabic",
    "fr": "French",
    "es": "Spanish",
    "hi": "Hindi",
    "ru": "Russian",
    "id": "Indonesian",
    "sw": "Swahili",
    "pt": "Portuguese",
    "zh": "Mandarin",
    "nl": "Dutch"
}

LANGUAGE_UNKNOWN = "UNKNOWN"

RELEVANCE_CUTOFF = .6
TOP_K = 10

logger = get_logger(__name__)


class Language(str, Enum):
    """Supported ISO 639-1 language codes for responses/messages."""
    ENGLISH = "en"
    ARABIC = "ar"
    FRENCH = "fr"
    SPANISH = "es"
    HINDI = "hi"
    RUSSIAN = "ru"
    INDONESIAN = "id"
    SWAHILI = "sw"
    PORTUGUESE = "pt"
    MANDARIN = "zh"
    DUTCH = "nl"
    OTHER = "Other"


class ResponseLanguage(BaseModel):
    """Model for parsing/validating the detected response language."""
    language: Language


class MessageLanguage(BaseModel):
    """Model for parsing/validating the detected language of a message."""
    language: Language


class PreprocessorResult(BaseModel):
    """Result type for the preprocessor node output."""
    new_message: str
    reason_for_decision: str
    message_changed: bool


class IntentType(str, Enum):
    """Enumeration of all supported user intents in the graph."""
    GET_BIBLE_TRANSLATION_ASSISTANCE = "get-bible-translation-assistance"
    PERFORM_UNSUPPORTED_FUNCTION = "perform-unsupported-function"
    RETRIEVE_SYSTEM_INFORMATION = "retrieve-system-information"
    SET_RESPONSE_LANGUAGE = "set-response-language"
    CONVERSE_WITH_BT_SERVANT = 'converse-with-bt-servant'


class UserIntents(BaseModel):
    """Container for a list of user intents."""
    intents: List[IntentType]


class BrainState(TypedDict, total=False):
    """State carried through the LangGraph execution."""
    user_id: str
    user_query: str
    query_language: str
    user_response_language: str
    transformed_query: str
    docs: List[Dict[str, str]]
    collection_used: str
    responses: Annotated[List[str], operator.add]
    translated_responses: List[str]
    stack_rank_collections: List[str]
    user_chat_history: List[Dict[str, str]]
    user_intents: UserIntents


def start(state: Any) -> dict:
    """Handle first interaction greeting, otherwise no-op."""
    s = cast(BrainState, state)
    user_id = s["user_id"]
    if is_first_interaction(user_id):
        set_first_interaction(user_id, False)
        return {"responses": [
            {"intent": "first-interaction", "response": FIRST_INTERACTION_MESSAGE}]}
    return {}


def determine_intents(state: Any) -> dict:
    """Classify the user's transformed query into one or more intents."""
    s = cast(BrainState, state)
    query = s["transformed_query"]
    messages: list[EasyInputMessageParam] = [
        {
            "role": "system",
            "content": INTENT_CLASSIFICATION_AGENT_SYSTEM_PROMPT,
        },
        {
            "role": "user",
            "content": f"what is your classification of the latest user message: {query}",
        },
    ]
    response = open_ai_client.responses.parse(
        model="gpt-4o",
        input=messages,
        text_format=UserIntents,
        store=False
    )
    user_intents = response.output_parsed
    logger.info("extracted user intents: %s", ' '.join([i.value for i in user_intents.intents]))

    return {
        "user_intents": user_intents.intents,
    }


def set_response_language(state: Any) -> dict:
    """Detect and persist the user's desired response language."""
    s = cast(BrainState, state)
    chat_input: list[EasyInputMessageParam] = [
        {
            "role": "user",
            "content": f"Past conversation: {json.dumps(s['user_chat_history'])}",
        },
        {
            "role": "user",
            "content": f"the user's most recent message: {s['user_query']}",
        },
        {
            "role": "user",
            "content": "What language is the user trying to set their response language to?",
        },
    ]
    response = open_ai_client.responses.parse(
        model="gpt-4o",
        input=chat_input,
        text_format=ResponseLanguage,
        store=False
    )
    resp_lang: ResponseLanguage = response.output_parsed
    if resp_lang.language == Language.OTHER:
        supported_language_list = ", ".join(supported_language_map.keys())
        response_text = (f"I think you're trying to set the response language. The supported languages "
                         f"are: {supported_language_list}. If this is your intent, please clearly tell "
                         f"me which supported language to use when responding.")
        return {"responses": [{"intent": IntentType.SET_RESPONSE_LANGUAGE, "response": response_text}]}
    user_id: str = s["user_id"]
    response_language_code: str = resp_lang.language.value
    set_user_response_language(user_id, response_language_code)
    language_name: str = supported_language_map.get(response_language_code, response_language_code)
    response_text = f"Setting response language to: {language_name}"
    return {
        "responses": [{"intent": IntentType.SET_RESPONSE_LANGUAGE, "response": response_text}],
        "user_response_language": response_language_code
    }


def combine_responses(chat_history, latest_user_message, responses) -> str:
    """Ask OpenAI to synthesize multiple node responses into one coherent text."""
    uncombined_responses = json.dumps(responses)
    logger.info("preparing to combine responses:\n\n%s", uncombined_responses)
    messages: list[EasyInputMessageParam] = [
        {
            "role": "developer",
            "content": f"conversation history: {chat_history}",
        },
        {
            "role": "developer",
            "content": f"latest user message: {latest_user_message}",
        },
        {
            "role": "developer",
            "content": f"responses to synthesize: {uncombined_responses}",
        },
    ]
    response = open_ai_client.responses.create(
        model="gpt-4o",
        instructions=COMBINE_RESPONSES_SYSTEM_PROMPT,
        input=messages,
    )
    combined = response.output_text
    logger.info("combined response from openai: %s", combined)
    return combined


def translate_responses(state: Any) -> dict:
    """Translate the response(s) into the user's desired language if needed."""
    s = cast(BrainState, state)
    uncombined_responses = list(s["responses"])
    num_responses = len(uncombined_responses)
    if num_responses > 1:
        query = s["user_query"]
        chat_history = s["user_chat_history"]
        responses = [combine_responses(chat_history, query, uncombined_responses)]
    elif num_responses == 1:
        responses = [uncombined_responses[0]["response"]]
    else:
        raise ValueError("no responses to translate. something bad happened. bailing out.")

    user_response_language = s["user_response_language"]
    if user_response_language:
        target_language = user_response_language
    else:
        target_language = s["query_language"]
        if target_language == LANGUAGE_UNKNOWN:
            logger.warning('target language unknown. bailing out.')
            supported_language_list = ", ".join(supported_language_map.keys())
            responses.append(("You haven't set your desired response language and I wasn't able to determine the "
                              "language of your original message in order to match it. You can set your desired "
                              "response language at any time by saying: Set my response language to Spanish, or "
                              f"Indonesian, or any of the supported languages: {supported_language_list}."))
            return {"translated_responses": responses}

    translated_responses = []
    for response in responses:
        response_language = detect_language(response)
        if response_language != target_language:
            logger.warning("target language: %s but response language: %s", target_language, response_language)
            logger.info('preparing to translate to %s', target_language)
            translated_responses.append(translate_text(response_text=response, target_language=target_language))
        else:
            logger.info('chunk translation not required. using chunk as is.')
            translated_responses.append(response)
    return {
        "translated_responses": translated_responses
    }


def translate_text(response_text, target_language):
    """Translate a single text into the target ISO 639-1 language code."""
    chat_messages: list[ChatCompletionMessageParam] = [
        {
            "role": "system",
            "content": RESPONSE_TRANSLATOR_SYSTEM_PROMPT,
        },
        {
            "role": "user",
            "content": (
                f"text to translate: {response_text}\n\n"
                f"ISO 639-1 code representing target language: {target_language}"
            ),
        },
    ]
    completion = open_ai_client.chat.completions.create(
        model="gpt-4o",
        messages=chat_messages,
    )
    translated_text = completion.choices[0].message.content
    logger.info('chunk: \n%s\n\ntranslated to:\n%s', response_text, translated_text)
    return translated_text


def detect_language(text) -> str:
    """Detect ISO 639-1 language code of the given text via OpenAI."""
    messages: list[EasyInputMessageParam] = [
        {
            "role": "system",
            "content": DETECT_LANGUAGE_AGENT_SYSTEM_PROMPT,
        },
        {
            "role": "user",
            "content": f"text: {text}",
        },
    ]
    response = open_ai_client.responses.parse(
        model="gpt-4o",
        input=messages,
        text_format=MessageLanguage,
        store=False
    )
    message_language = response.output_parsed
    return message_language.language.value


def determine_query_language(state: Any) -> dict:
    """Determine the language of the user's original query and set collection order."""
    s = cast(BrainState, state)
    query = s["user_query"]
    query_language = detect_language(query)
    logger.info("language code %s detected by gpt-4o.", query_language)
    stack_rank_collections = [
        "knowledgebase",
        "bsb",
        "tyndale_dictionary",
        "uw_translation_words",
        "uw_translation_notes",
        "biblica_study_notes_key_terms"
    ]

    return {
        "query_language": query_language,
        "stack_rank_collections": stack_rank_collections
    }


def preprocess_user_query(state: Any) -> dict:
    """Lightly clarify or correct the user's query using conversation history."""
    s = cast(BrainState, state)
    query = s["user_query"]
    chat_history = s["user_chat_history"]
    history_context_message = f"past_conversation: {json.dumps(chat_history)}"
    messages: list[EasyInputMessageParam] = [
        {
            "role": "user",
            "content": history_context_message,
        },
        {
            "role": "user",
            "content": f"current_message: {query}",
        },
    ]
    response = open_ai_client.responses.parse(
        model="gpt-4o",
        instructions=PREPROCESSOR_AGENT_SYSTEM_PROMPT,
        input=messages,
        text_format=PreprocessorResult,
        store=False
    )
    preprocessor_result = response.output_parsed
    new_message = preprocessor_result.new_message
    reason_for_decision = preprocessor_result.reason_for_decision
    message_changed = preprocessor_result.message_changed
    logger.info("new_message: %s\nreason_for_decision: %s\nmessage_changed: %s",
                new_message, reason_for_decision, message_changed)
    return {
        "transformed_query": new_message if message_changed else query
    }


def query_vector_db(state: Any) -> dict:
    """Query the vector DB (Chroma) across ranked collections and filter by relevance."""
    # pylint: disable=too-many-locals
    s = cast(BrainState, state)
    query = s["transformed_query"]
    stack_rank_collections = s["stack_rank_collections"]
    filtered_docs = []
    # this loop is the current implementation of the "stacked ranked" algorithm
    for collection_name in stack_rank_collections:
        logger.info("querying stack collection: %s", collection_name)
        db_collection = get_chroma_collection(collection_name)
        if not db_collection:
            logger.warning("collection %s was not found in chroma db.", collection_name)
            continue
        results = db_collection.query(
            query_texts=[query],
            n_results=TOP_K
        )
        docs = results["documents"]
        similarities = results["distances"]
        metadata = results["metadatas"]
        logger.info("\nquery: %s\n", query)
        logger.info("---")
        hits = 0
        for i in range(len(docs[0])):
            cosine_similarity = round(1 - similarities[0][i], 4)
            doc = docs[0][i]
            m = metadata[0][i]
            resource_name = m.get("name", "")
            source = m.get("source", "")
            logger.info("processing %s from %s.", resource_name, source)
            logger.info("Cosine Similarity: %s", cosine_similarity)
            logger.info("Metadata: %s", resource_name)
            logger.info("---")
            if cosine_similarity >= RELEVANCE_CUTOFF:
                hits += 1
                filtered_docs.append({
                    "collection_name": collection_name,
                    "resource_name": resource_name,
                    "source": source,
                    "document_text": doc
                })
        if hits > 0:
            logger.info("found %d hit(s) at stack collection: %s", hits, collection_name)

    return {
        "docs": filtered_docs
    }


# pylint: disable=too-many-locals
def query_open_ai(state: Any) -> dict:
    """Generate the final response text using RAG context and OpenAI."""
    s = cast(BrainState, state)
    docs = s["docs"]
    query = s["transformed_query"]
    chat_history = s["user_chat_history"]
    try:
        if len(docs) == 0:
            no_docs_msg = (f"Sorry, I couldn't find any information in my resources to service your request "
                           f"or command.\n\n{BOILER_PLATE_AVAILABLE_FEATURES_MESSAGE}")
            return {"responses": [
                {"intent": IntentType.GET_BIBLE_TRANSLATION_ASSISTANCE, "response": no_docs_msg}]}

        # build context from docs
        # context = "\n\n".join([item["doc"] for item in docs])
        context = json.dumps(docs, indent=2)
        logger.info("context passed to final node:\n\n%s", context)
        rag_context_message = "When answering my next query, use this additional" + \
            f"  context: {context}"
        chat_history_context_message = (f"Use this conversation history to understand the user's "
                                        f"current request only if needed: {json.dumps(chat_history)}")
        response = open_ai_client.responses.create(
            model="gpt-4o",
            instructions=FINAL_RESPONSE_AGENT_SYSTEM_PROMPT,
            input=[
                {
                    "role": "developer",
                    "content": rag_context_message
                },
                {
                    "role": "developer",
                    "content": chat_history_context_message
                },
                {
                    "role": "user",
                    "content": query
                }
            ]
        )
        bt_servant_response = response.output_text
        logger.info('response from openai: %s', bt_servant_response)
        logger.debug("%d characters returned from openAI", len(bt_servant_response))

        resource_list = ", ".join({
            f"{item.get('resource_name', 'unknown')} from {item.get('source', 'unknown')}"
            for item in docs
        })
        cascade_info = (
            f"bt servant used the following resources to generate its response: {resource_list}."
        )
        logger.info(cascade_info)

        return {"responses": [
            {"intent": IntentType.GET_BIBLE_TRANSLATION_ASSISTANCE, "response": bt_servant_response}]}
    except OpenAIError:
        logger.error("Error during OpenAI request", exc_info=True)
        error_msg = "I encountered some problems while trying to respond. Let Ian know about this one."
        return {"responses": [{"intent": IntentType.GET_BIBLE_TRANSLATION_ASSISTANCE, "response": error_msg}]}


def chunk_message(state: Any) -> dict:
    """Chunk oversized responses to respect WhatsApp limits, via LLM or fallback."""
    logger.info("MESSAGE TOO BIG. CHUNKING...")
    s = cast(BrainState, state)
    responses = s["translated_responses"]
    text_to_chunk = responses[0]
    chunk_max = config.MAX_META_TEXT_LENGTH - 100
    try:
        chat_messages: list[ChatCompletionMessageParam] = [
            {
                "role": "system",
                "content": CHOP_AGENT_SYSTEM_PROMPT,
            },
            {
                "role": "user",
                "content": f"text to chop: \n\n{text_to_chunk}",
            },
        ]
        completion = open_ai_client.chat.completions.create(
            model='gpt-4o',
            messages=chat_messages,
        )
        response = completion.choices[0].message.content
        chunks = json.loads(response)
    except json.JSONDecodeError:
        logger.error("Error while attempting to chunk message. Manually chunking instead", exc_info=True)
        chunks = chop_text(text=text_to_chunk, n=chunk_max)

    chunks.extend(responses[1:])
    return {"translated_responses": combine_chunks(chunks=chunks, chunk_max=chunk_max)}


def needs_chunking(state: BrainState) -> str:
    """Return next node key if chunking is required, otherwise finish."""
    first_response = state["translated_responses"][0]
    if len(first_response) > config.MAX_META_TEXT_LENGTH:
        logger.warning('message to big: %d chars. preparing to chunk.', len(first_response))
        return "chunk_message_node"
    return END


def process_intents(state: Any) -> List[str]:
    """Map detected intents to the list of nodes to traverse."""
    s = cast(BrainState, state)
    user_intents = s["user_intents"]
    if not user_intents:
        raise ValueError("no intents found. something went very wrong.")

    nodes_to_traverse = []
    if IntentType.GET_BIBLE_TRANSLATION_ASSISTANCE in user_intents:
        nodes_to_traverse.append("query_vector_db_node")
    if IntentType.SET_RESPONSE_LANGUAGE in user_intents:
        nodes_to_traverse.append("set_response_language_node")
    if IntentType.PERFORM_UNSUPPORTED_FUNCTION in user_intents:
        nodes_to_traverse.append("handle_unsupported_function_node")
    if IntentType.RETRIEVE_SYSTEM_INFORMATION in user_intents:
        nodes_to_traverse.append("handle_system_information_request_node")
    if IntentType.CONVERSE_WITH_BT_SERVANT in user_intents:
        nodes_to_traverse.append("converse_with_bt_servant_node")

    return nodes_to_traverse


def handle_unsupported_function(state: Any) -> dict:
    """Generate a helpful response when the user requests unsupported functionality."""
    s = cast(BrainState, state)
    query = s["user_query"]
    chat_history = s["user_chat_history"]
    messages: list[EasyInputMessageParam] = [
        {
            "role": "developer",
            "content": f"Conversation history to use if needed: {json.dumps(chat_history)}",
        },
        {
            "role": "user",
            "content": query,
        },
    ]
    response = open_ai_client.responses.create(
        model="gpt-4o",
        instructions=UNSUPPORTED_FUNCTION_AGENT_SYSTEM_PROMPT,
        input=messages,
        store=False
    )
    unsupported_function_response_text = response.output_text
    logger.info('converse_with_bt_servant response from openai: %s', unsupported_function_response_text)
    return {"responses": [{"intent": IntentType.PERFORM_UNSUPPORTED_FUNCTION, "response": unsupported_function_response_text}]}


def handle_system_information_request(state: Any) -> dict:
    """Provide help/about information for the BT Servant system."""
    s = cast(BrainState, state)
    query = s["user_query"]
    chat_history = s["user_chat_history"]
    messages: list[EasyInputMessageParam] = [
        {
            "role": "developer",
            "content": f"Conversation history to use if needed: {json.dumps(chat_history)}",
        },
        {
            "role": "user",
            "content": query,
        },
    ]
    response = open_ai_client.responses.create(
        model="gpt-4o",
        instructions=HELP_AGENT_SYSTEM_PROMPT,
        input=messages,
        store=False
    )
    help_response_text = response.output_text
    logger.info('help response from openai: %s', help_response_text)
    return {"responses": [{"intent": IntentType.RETRIEVE_SYSTEM_INFORMATION, "response": help_response_text}]}


def converse_with_bt_servant(state: Any) -> dict:
    """Respond conversationally to the user based on context and history."""
    s = cast(BrainState, state)
    query = s["user_query"]
    chat_history = s["user_chat_history"]
    messages: list[EasyInputMessageParam] = [
        {
            "role": "developer",
            "content": f"Conversation history to use if needed: {json.dumps(chat_history)}",
        },
        {
            "role": "user",
            "content": query,
        },
    ]
    response = open_ai_client.responses.create(
        model="gpt-4o",
        instructions=CONVERSE_AGENT_SYSTEM_PROMPT,
        input=messages,
        store=False
    )
    converse_response_text = response.output_text
    logger.info('converse_with_bt_servant response from openai: %s', converse_response_text)
    return {"responses": [{"intent": IntentType.CONVERSE_WITH_BT_SERVANT, "response": converse_response_text}]}


def create_brain():
    """Assemble and compile the LangGraph for the BT Servant brain."""
    builder: StateGraph[BrainState] = StateGraph(BrainState)

    builder.add_node("start_node", start)
    builder.add_node("determine_query_language_node", determine_query_language)
    builder.add_node("preprocess_user_query_node", preprocess_user_query)
    builder.add_node("determine_intents_node", determine_intents)
    builder.add_node("set_response_language_node", set_response_language)
    builder.add_node("query_vector_db_node", query_vector_db)
    builder.add_node("query_open_ai_node", query_open_ai)
    builder.add_node("chunk_message_node", chunk_message)
    builder.add_node("handle_unsupported_function_node", handle_unsupported_function)
    builder.add_node("handle_system_information_request_node", handle_system_information_request)
    builder.add_node("converse_with_bt_servant_node", converse_with_bt_servant)
    builder.add_node("translate_responses_node", translate_responses, defer=True)

    builder.set_entry_point("start_node")
    builder.add_edge("start_node", "determine_query_language_node")
    builder.add_edge("determine_query_language_node", "preprocess_user_query_node")
    builder.add_edge("preprocess_user_query_node", "determine_intents_node")
    builder.add_conditional_edges(
        "determine_intents_node",
        process_intents
    )
    builder.add_edge("query_vector_db_node", "query_open_ai_node")
    builder.add_edge("set_response_language_node", "translate_responses_node")
    builder.add_edge("chunk_message_node", "translate_responses_node")

    builder.add_edge("handle_unsupported_function_node", "translate_responses_node")
    builder.add_edge("handle_system_information_request_node", "translate_responses_node")
    builder.add_edge("converse_with_bt_servant_node", "translate_responses_node")
    builder.add_edge("query_open_ai_node", "translate_responses_node")

    builder.add_conditional_edges(
        "translate_responses_node",
        needs_chunking
    )
    builder.set_finish_point("chunk_message_node")

    return builder.compile()
