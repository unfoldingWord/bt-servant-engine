"""Decision graph and message-processing pipeline for BT Servant.

This module defines the state, nodes, and orchestration logic for handling
incoming user messages, classifying intents, querying resources, and producing
final responses (including translation and chunking when necessary).
"""
# pylint: disable=line-too-long,too-many-lines,too-many-statements

from __future__ import annotations

import json
import operator
from pathlib import Path
import re
from typing import Annotated, Dict, List, cast, Any, Optional
from collections.abc import Hashable
from enum import Enum
from typing_extensions import TypedDict

from openai import OpenAI, OpenAIError
from openai.types.responses.easy_input_message_param import EasyInputMessageParam
from openai.types.chat.chat_completion_message_param import ChatCompletionMessageParam
from langgraph.graph import END, StateGraph
from pydantic import BaseModel, ValidationError

from logger import get_logger
from config import config
from utils import chop_text, combine_chunks
from utils.bsb import (
    BOOK_MAP as BSB_BOOK_MAP,
    normalize_book_name,
    select_verses,
    label_ranges,
    clamp_ranges_by_verse_limit,
    parse_ch_verse_from_reference,
)
from utils.bible_data import resolve_bible_data_root, list_available_sources
from utils.keywords import select_keywords
from utils.translation_helps import select_translation_helps, get_missing_th_books
from utils.bible_locale import get_book_name
from utils.perf import time_block, set_current_trace, add_tokens
from db import (
    get_chroma_collection,
    is_first_interaction,
    set_first_interaction,
    set_user_response_language,
)

FEATURES_SUMMARY_RESPONSE = (
    "Currently, I can do three main things: summarize a passage, provide key words in a "
    "passage, or provide the typical translation challenges found in a passage. Here are some "
    "example questions or commands corresponding to these three functions: "
)
BOILER_PLATE_AVAILABLE_FEATURES_MESSAGE = f"""
    {FEATURES_SUMMARY_RESPONSE}

    (1) Please summarize Titus chapter 1.
    (2) List all the important words in Romans 1.
    (3) What challenges might I face when translating John 1:1?

    Which of these would you like me to do?
"""

FIRST_INTERACTION_MESSAGE = f"""
Hello! I am the BT Servant. This is our first conversation. Let's work together to understand and translate God's word!

{BOILER_PLATE_AVAILABLE_FEATURES_MESSAGE}
"""

UNSUPPORTED_FUNCTION_AGENT_SYSTEM_PROMPT = f"""
# Identity

You are a part of a RAG bot system that assists Bible translators. You are one node in the decision/intent processing 
lang graph. Specifically, your job is to handle the perform-unsupported-function intent. This means the user is trying 
to perform an unsupported function.

# Instructions

Respond appropriately to the user's request to do something that you currently can't do. Leverage the 
user's message and the conversation history if needed. Make sure to always end your response with some version of  
the boiler plate available features message (see below).

<boiler_plate_available_features_message>
    {BOILER_PLATE_AVAILABLE_FEATURES_MESSAGE}
</boiler_plate_available_features_message>
"""

COMBINE_RESPONSES_SYSTEM_PROMPT = """
# Identity

You are a part of a RAG bot system that assists Bible translators. The decision system is a lang graph with various 
nodes handling multiple user intents. Your job is to combine the response messages from various intent processing 
nodes in the graph into one cohesive message that makes sense.

# Instructions

You will be given a json array of objects. Each object will have two properties: (1) the intent of the intent 
processing node that generated the message. (2) the response message itself. In general, your job is to return a single 
string representing the combined message. The combined message should be natural sounding, cohesive, and, to the degree 
possible, contain all the elements of the individual messages. You will also be given the conversation history and the 
user's most recent message. Leverage this context when combining response messages! Below are six guidelines for you to 
use when combining messages:

(1) if the first-interaction intent was processed, the information and message related to this intent SHOULD ALWAYS 
COME FIRST!!!

(2) If the CONVERSE_WITH_BT_SERVANT intent was processed, the combined message should usually start with some version of
the response message generated by this intent processing node. The only thing that should ever go before this is the 
information and message related to the "first-interaction" intent.

(3) If the SET_RESPONSE_LANGUAGE intent was processed, the combined message should usually end with some version of 
the response message generated by this intent processing node.

(4) If the GET_BIBLE_TRANSLATION_ASSISTANCE intent, or the GET_PASSAGE_SUMMARY intent, was processed, the information 
contained in the response message generated by these intent processing nodes should usually be as close to the 
beginning as possible, unless that would violate guideline #1 above. 

(5) If some combination of the PERFORM_UNSUPPORTED_FUNCTION and RETRIEVE_SYSTEM_INFORMATION intents were processed, the 
information from the associated response messages should usually fall in the middle somewhere. 

(6) Make sure to synthesize/remove any repeated or redundant information. This is very important!!!

(7) If there are multiple questions found in the various responses, these must be reduced to one question, and that 
question must be at the end of the message. Any question in the combined response must come at the very end of the 
message.

(8) If you detect in conversation history that you've already said hello, there's no need to say it again.

(9) If it doesn't make sense to say "hello!" to the user, based on their most recent message, there's no need to say 
'Hello!  I'm here to assist with Bible translation tasks' again.

(10) Remove duplicated boilerplate or repeated feature lists if multiple nodes include similar guidance; keep only one
concise version where appropriate.

Don't worry about the combined response being too big. A downstream node will chunk the message if needed.
"""

CONVERSE_AGENT_SYSTEM_PROMPT = f"""
# Identity

You are a part of a RAG bot system that assists Bible translators. You are one node in the decision/intent processing 
lang graph. Specifically, your job is to handle the converse-with-bt-servant intent by responding conversationally to 
the user based on the provided context.

# Instructions

If we are here in the decision graph, the converse-with-bt-servant intent has been detected. You will be provided with 
the user's most recent message and conversation history. Your job is to respond conversationally to the user. Unless it 
doesn't make sense to do so, aim to end your response with some version of  the boiler plate available features message 
(see below).

<boiler_plate_available_features_message>
    {BOILER_PLATE_AVAILABLE_FEATURES_MESSAGE}
</boiler_plate_available_features_message>
"""

HELP_AGENT_SYSTEM_PROMPT = """
# Identity

You are a part of a WhatsApp RAG bot system that assists Bible translators called BT Servant. You sole purpose is to 
provide help information about the BT Servant system. If this node has been hit, it means the system has already 
classified the user's most recent message as a desire to receive help or more information about the system. This is 
typically the result of them saying something like: 'help!' or 'tell me about yourself' or 'how does this work?' Thus, 
make sure to always provide some help, to the best of your abilities. Always provide help to the user.

# Instructions
You will be supplied with the user's most recent message and also past conversation history. Using this context, 
provide the user with information detailing how the system works (the features of the BT Servant system). When doing 
so, leverage the feature information below. In almost all situations, when appropriate, end your response with a 
question asking the user if they want to use one of the first three features (example: 'Would you like me to summarize 
a verse or passage for you?').

# Features

1. Summarize books or passages of Scripture. (Example usage: 'please summarize Titus 2'; Please summarize Mark 1:1-8.)

2. Provide translation issues found in a verse range, chapter, or book. (Example usage: 'Tell me all the translation 
challenges in Mark 1:2.')

3. Provide a list of keywords found in a verse range, chapter, or book. (Example usage: 'What are all the keywords in 
Titus 1:5-16?')

4. Set the response language for the user. This is a persistent setting for the user and determines the response 
language of the system. If this is not set, the system tries to respond using the same language as the user's message. 
Currently, the supported languages are: English, Arabic, French, Spanish, Hindi, Russian, Indonesian, Swahili, 
Portuguese, Mandarin, and Dutch. (Example usage: Please set my response language to Spanish.)

# Using prior history for better responses

Here are some guidelines for using history for better responses:
1. If you detect in conversation history that you've already said hello, there's no need to say it again.
2. If it doesn't make sense to say "hello!" to the user, based on their most recent message, there's no need to say 
'Hello!  I'm here to assist with Bible translation tasks' again.
"""


RESPONSE_TRANSLATOR_SYSTEM_PROMPT = """
    You are a translator for the final output in a chatbot system. You will receive text that 
    needs to be translated into the language represented by the specified ISO 639-1 code.
"""

TRANSLATE_PASSAGE_AGENT_SYSTEM_PROMPT = """
# Task

Translate the provided scripture passage into the specified target language and return a STRICT JSON object
matching the provided schema. Do not include any extra prose, commentary, code fences, or formatting.

# Rules
- header_book: translate ONLY the canonical book name into the target language (e.g., "John" -> "Иоанн").
- header_suffix: DO NOT translate or alter; copy exactly the provided suffix (e.g., "1:1–7").
- body: translate the passage body into the target language; PRESERVE all newline boundaries exactly; do not add
  bullets, numbers, verse labels, or extra headings.
- content_language: the ISO 639-1 code of the target language.

# Output
Return JSON matching the schema with fields: header_book, header_suffix, body, content_language. No extra keys.
"""


PREPROCESSOR_AGENT_SYSTEM_PROMPT = """
# Identity

You are a preprocessor agent/node in a retrieval augmented generation (RAG) pipeline. 

# Instructions

Use past conversation context, 
if supplied and applicable, to disambiguate or clarify the intent or meaning of the user's current message. Change 
as little as possible. Change nothing unless necessary. If the intent of the user's message is already clear, 
change nothing. Never greatly expand the user's current message. Changes should be small or none. Feel free to fix 
obvious spelling mistakes or errors, but not logic errors like incorrect books of the Bible. Do NOT narrow the scope of
explicit scripture selections: if a user requests multiple chapters, verse ranges, or disjoint selections (including
conjunctions like "and" or comma/semicolon lists), preserve them exactly as written. If the system has constraints
(for example, only a single chapter can be processed at a time), do NOT modify the user's message to fit those
constraints — leave the message intact and let downstream nodes handle any rejection or guidance. For translation
requests, do NOT add or change a target language; preserve only what the user explicitly stated.
Return the clarified 
message and the reasons for clarifying or reasons for not changing anything. Examples below.

# Examples

## Example 1

<past_conversation>
    user_message: Summarize the book of Titus.
    assistant_response: The book of titus is about...
</past_conversation>

<current_message>
    user_message: Now Mark
</current_message>

<assistant_response>
    new_message: Now Summarize the book of Mark.
    reason_for_decision: Based on previous context, the user wants the system to do the same thing, but this time 
                         with Mark.
    message_changed: True
</assistant_response>
    
## Example 2

<past_conversation>
    user_message: What is going on in 1 Peter 3:7?
    assistant_response: Peter is instructing Christian husbands to be loving to their wives.
</past_conversation>

<current_message>
    user_message: Summarize Mark 3:1
</current_message>
    
<assistant_response>
    new_message: Summarize Mark 3:1.
    reason_for_decision: Nothing was changed. The user's current command has nothing to do with past context and
                         is fine as is.
    message_changed: False
</assistant_response>

## Example 3

<past_conversation>
    user_message: Explain John 1:1
    assistant_response: John claims that Jesus, the Word, existed in the beginning with God the Father.
</past_conversation>

<current_message>
    user_message: Explain John 1:3
</current_message>
    
<assistant_response>
    new_message: Explain John 1:3.
    reason_for_decision: The word 'John' was misspelled in the message.
    message_changed: True
</assistant_response>
"""


PASSAGE_SELECTION_AGENT_SYSTEM_PROMPT = """
# Identity

You classify the user's message to extract explicit Bible passage references.
Return a normalized, structured selection of book + verse ranges.

# Instructions

- Only choose from these canonical book names (exact match):
  {books}
- Accept a variety of phrasings (e.g., "John 3:16", "Jn 3:16–18", "1 John 2:1-3", "Psalm 1", "Song of Songs 2").
- Normalize all book names to the exact canonical name.
- Numbered books are distinct canonical names. When a leading number precedes a book
  (e.g., "1 John", "2 Samuel", "3 John"), treat the number as part of the book name —
  it is NOT a chapter reference. For example, "3 John" means the book "3 John"
  (and the whole book if no chapter/verse is given), whereas "John 3" means chapter 3 of "John".
- Support:
  - Single verse (John 3:16)
  - Verse ranges within a chapter (John 3:16-18)
  - Cross-chapter within a single book (John 3:16–4:2)
  - Whole chapters (John 3)
  - Multi-chapter ranges with no verse specification (e.g., "John 1–4", "John chapters 1–4"): set start_chapter=1, end_chapter=4 and leave verses empty
  - Whole book (John)
  - Multiple disjoint ranges within the same book (comma/semicolon separated)
- Do not cross books in one selection. If the user mentions multiple books (including with 'and', commas, or hyphens like 'Gen–Exo') and a single book cannot be unambiguously inferred by explicit chapter/verse qualifiers, return an empty selection. Prefer a clearly qualified single book (e.g., "Mark 1:1") over earlier mentions without qualifiers.
- If no verses/chapters are supplied for the chosen book, interpret it as the whole book.
- If no clear passage is present (and no book can be reasonably inferred), return an empty list.

# Output format
Return JSON parsable into the provided schema.

# Examples
- "What are the keywords in Genesis and Exodus?" -> return empty selection (multiple books; no clear single-book qualifier).
- "Gen–Exo" -> return empty selection (multiple books; no clear single-book qualifier).
- "John and Mark 1:1" -> choose Mark 1:1 (explicit qualifier picks Mark over first mention).
- "summarize 3 John" -> choose book "3 John" with no chapters/verses (whole book selection).
- "summarize John 3" -> choose book "John" with start_chapter=3 (whole chapter if no verses).
"""


PASSAGE_SUMMARY_AGENT_SYSTEM_PROMPT = """
You summarize Bible passage content faithfully using only the verses provided.

- Stay strictly within the supplied passage text; avoid speculation or doctrinal claims not present in the text.
- Highlight the main flow, key ideas, and important movements or contrasts across the entire selection.
- Provide a thorough, readable summary (not terse). Aim for roughly 8–15 sentences, but expand if the selection is large.
- Write in continuous prose only: do NOT use bullets, numbered lists, section headers, or list-like formatting. Compose normal paragraph(s) with sentences flowing naturally.
- Mix verse references inline within the prose wherever helpful (e.g., "1:1–3", "3:16", "2:4–6") to anchor key points rather than isolating them as list items.
- If the selection contains only a single verse, inline verse references are not necessary.
"""


FINAL_RESPONSE_AGENT_SYSTEM_PROMPT = """
You are an assistant to Bible translators. Your main job is to answer questions about content found in various biblical 
resources: commentaries, translation notes, bible dictionaries, and various resources like FIA. In addition to answering
questions, you may be called upon to: summarize the data from resources, transform the data from resources (like
explaining it a 5-year old level, etc, and interact with the resources in all kinds of ways. All this is a part of your 
responsibilities. Context from resources (RAG results) will be provided to help you answer the question(s). Only answer 
questions using the provided context from resources!!! If you can't confidently figure it out using that context, 
simply say 'Sorry, I couldn't find any information in my resources to service your request or command. But 
maybe I'm unclear on your intent. Could you perhaps state it a different way?' You will also be given the past 
conversation history. Use this to understand the user's current message or query if necessary. If the past conversation 
history is not relevant to the user's current message, just ignore it. FINALLY, UNDER NO CIRCUMSTANCES ARE YOU TO SAY 
ANYTHING THAT WOULD BE DEEMED EVEN REMOTELY HERETICAL BY ORTHODOX CHRISTIANS. If you can't do what the user is asking 
because your response would be heretical, explain to the user why you cannot comply with their request or command.
"""

CHOP_AGENT_SYSTEM_PROMPT = (
    "You are an agent tasked to ensure that a message intended for Whatsapp fits within the 1500 character limit. Chop "
    "the supplied text in the biggest possible semantic chunks, while making sure no chuck is >= 1500 characters. "
    "Your output should be a valid JSON array containing strings (wrapped in double quotes!!) constituting the chunks. "
    "Only return the json array!! No ```json wrapper or the like. Again, make chunks as big as possible!!!"
)

INTENT_CLASSIFICATION_AGENT_SYSTEM_PROMPT = """
You are a node in a chatbot system called “BT Servant”, which provides intelligent assistance to Bible translators. Your 
job is to classify the **intent(s)** of the user’s latest message. Always return **at least one** intent from the 
approved list. However, if more than one intent is found, make sure to return those as well. If you're unsure, return 
`perform-unsupported-function`. If the user is asking for something outside the scope of the Bible, Bible translation, 
the Bible translation process, or one of the resources stored in the system (ex. Translation Notes, FIA resources, 
the Bible, Translation Words, Greek or Hebrew resources, commentaries, Bible dictionaries, etc.), or something outside 
system capabilities (defined by the various intents), also return the `perform-unsupported-function` intent.

You MUST always return at least one intent. You MUST choose one or more intents from the following intent types:

<intents>
  <intent name="get-bible-translation-assistance">
    The user is asking for help with Bible translation — including understanding meaning; finding source verses; 
    clarifying language issues; consulting translation resources (ex. Translation Notes, FIA, the Bible, etc); receiving
    explanation of resources; interacting with resource content; asking for transformations of resource content 
    (ex. summaries of resource portions, biblical content, etc); or how to handle specific words, phrases, 
    or translation challenges. This also includes asking about biblical people, places, things, or ideas.
  </intent>
  <intent name="get-passage-summary">
    The user is explicitly asking for a summary of a specific Bible passage, verse range, chapter(s), or entire book
    (e.g., "John 3:16-18", "John 1–4", "Summarize John"). Prefer this when the user clearly requests a summary.
    If the user mentions multiple books (e.g., "summarize John and Mark"), still classify as `get-passage-summary` —
    downstream logic will handle scope constraints.
  </intent>
  <intent name="get-passage-keywords">
    The user is explicitly asking for key words in a specific Bible passage, verse range, chapter(s),
    or entire book (e.g., "Hebrews 1:1–11", "Joel", "John 1–3"). Prefer this when the user clearly
    requests keywords, important words, or pivotal words to focus on during translation.
  </intent>
  <intent name="get-translation-helps">
    The user is asking for translation challenges, considerations, guidance, or alternate renderings for a given
    passage or book. Prefer this when the user mentions verse ranges and asks about "alternate translations",
    "other ways to translate", or translation options for specific words/phrases in context.
    Examples include: "Help me translate Titus 1:1–5", "translation challenges for Exo 1",
    "what to consider when translating the book of Ruth", "alternate translations for 'only begotten' in John 3:16",
    or "what are other ways to translate 'flesh' in Gal 5:19–21?".
  </intent>
  <intent name="retrieve-scripture">
    The user is asking for the exact text of a Bible passage (verbatim verse text), optionally specifying a
    language or Bible/version (e.g., "Give me John 1:1 in Indonesian", "Provide the text of Job 1:1-5"). Prefer this
    when the user wants the scripture text itself, not a summary or guidance.
  </intent>
  <intent name="translate-scripture">
    The user wants the Bible passage text translated into a specified target language, optionally from a specified
    source language or version (e.g., "translate John 1:1 into Portuguese", "translate the French version of John 1:1
    into Spanish"). Prefer this when the user asks to translate scripture itself.
  </intent>
  <intent name="set-response-language">
    The user wants to change the language in which the system responds. They might ask for responses in 
    Spanish, French, Arabic, etc.
  </intent>
  <intent name="retrieve-system-information">
    The user wants information about the BT Servant system itself — how it works, where it gets data, uptime, 
    example questions, supported languages, features, or current system configuration (like the documents currently 
    stored in the ChromaDB (vector database).
  </intent>
  <intent name="perform-unsupported-function">
    The user is asking BT Servant to do something outside the scope of Bible translation help, interacting with the 
    resources in the vector database, or system diagnostics. For example, telling jokes, setting timers, 
    summarizing current news, or anything else COMPLETELY UNRELATED to what BT Servant can do.
  </intent>
  <intent name="converse-with-bt-servant">
    The user is trying to talk to bt-servant (the bot/system). This represents any attempt to engage in conversation, 
    including simple greetings like: hello, hi, or even what's up! It also includes random conversation or statements 
    from the user. Essentially, this intent should be used if none of the other intent classifications make sense.
  </intent>
</intents>

Here are a few examples to guide you:

<examples>
  <example>
    <message>tell me about ephesus</message>
    <intent>get-bible-translation-assistance</intent>
  </example>
  <example>
    <message>tell me about Herod</message>
    <intent>get-bible-translation-assistance</intent>
  </example>
  <example>
    <message>What is a danarius?</message>
    <intent>get-bible-translation-assistance</intent>
  </example>
  <example>
    <message>What is the fourth step of the FIA process?</message>
    <intent>get-bible-translation-assistance</intent>
  </example>
  <example>
    <message>Explain the FIA process to me like I'm a three year old.</message>
    <intent>get-bible-translation-assistance</intent>
  </example>
  <example>
    <message>What is a FIA process in Mark.</message>
    <intent>get-bible-translation-assistance</intent>
  </example>
  <example>
    <message>Summarize Mark 3.</message>
    <intent>get-passage-summary</intent>
  </example>
  <example>
    <message>Summarize Titus 3:4</message>
    <intent>get-passage-summary</intent>
  </example>
  <example>
    <message>summarize John and Mark</message>
    <intent>get-passage-summary</intent>
  </example>
  <example>
    <message>summarize Gen–Exo</message>
    <intent>get-passage-summary</intent>
  </example>
  <example>
    <message>summarize Genesis and Exodus</message>
    <intent>get-passage-summary</intent>
  </example>
  <example>
    <message>what are the important words in EXO-DEUT</message>
    <intent>get-passage-keywords</intent>
  </example>
  <example>
    <message>keywords in Genesis and Exodus</message>
    <intent>get-passage-keywords</intent>
  </example>
  <example>
    <message>list the keywords for Gen 1-3</message>
    <intent>get-passage-keywords</intent>
  </example>
  <example>
    <message>What are all the important words in Hebrews 1:1-11?</message>
    <intent>get-passage-keywords</intent>
  </example>
  <example>
    <message>what are all the keywords in the book of Joel?</message>
    <intent>get-passage-keywords</intent>
  </example>
  <example>
    <message>what words are pivotal from a translation perspective in John 1-3</message>
    <intent>get-passage-keywords</intent>
  </example>
  <example>
    <message>What are alternate translations for "only begotten" in John 3:16?</message>
    <intent>get-translation-helps</intent>
  </example>
  <example>
    <message>What are other ways to translate "flesh" in Gal 5:19-21?</message>
    <intent>get-translation-helps</intent>
  </example>
  <example>
    <message>Alternate renderings in Rom 3:25-26</message>
    <intent>get-translation-helps</intent>
  </example>
  <example>
    <message>Help me translate Titus 1:1-5</message>
    <intent>get-translation-helps</intent>
  </example>
  <example>
    <message>What are some translation challenges I should consider when translating Exo 1?</message>
    <intent>get-translation-helps</intent>
  </example>
  <example>
    <message>What do I need to worry about when translating the book of Ruth?</message>
    <intent>get-translation-helps</intent>
  </example>
  <example>
    <message>Give me a summary of John 3:16-18.</message>
    <intent>get-passage-summary</intent>
  </example>
  <example>
    <message>translate John 1:1 into Portuguese</message>
    <intent>translate-scripture</intent>
  </example>
  <example>
    <message>translate the French version of John 1:1 into Spanish</message>
    <intent>translate-scripture</intent>
  </example>
  <example>
    <message>Please provide the text of Job 1:1-5</message>
    <intent>retrieve-scripture</intent>
  </example>
  <example>
    <message>Can you give me John 1:1 from the Indonesian Bible?</message>
    <intent>retrieve-scripture</intent>
  </example>
  <example>
    <message>Can you reply to me in French from now on?</message>
    <intent>set-response-language</intent>
  </example>
  <example>
    <message>Where does BT Servant get its information from?</message>
    <intent>retrieve-system-information</intent>
  </example>
  <example>
    <message>Help</message>
    <intent>retrieve-system-information</intent>
  </example>
  <example>
    <message>Can you tell me a joke?</message>
    <intent>perform-unsupported-function</intent>
  </example>
  <example>
    <message>Hmm, what was I saying again?</message>
    <intent>converse-with-bt-servant</intent>
  </example>
  <example>
    <message>hello</message>
    <intent>converse-with-bt-servant</intent>
  </example>
  <example>
    <message>hi</message>
    <intent>converse-with-bt-servant</intent>
  </example>
  <example>
    <message>what's up!</message>
    <intent>converse-with-bt-servant</intent>
  </example>
  <example>
    <message>Good morning</message>
    <intent>converse-with-bt-servant</intent>
  </example>
  <example>
    <message>How are you doing today?</message>
    <intent>converse-with-bt-servant</intent>
  </example>
</examples>

You will return a single structured output like this:
```json
{ "intents": ["get-bible-translation-assistance"] }
```
"""

DETECT_LANGUAGE_AGENT_SYSTEM_PROMPT = """
Task: Detect the language of the supplied user text and return the ISO 639-1 code from the allowed set.

Allowed outputs: en, ar, fr, es, hi, ru, id, sw, pt, zh, nl, Other

Bible context: Bible book abbreviations are language-neutral (e.g., Gen, Exo, Lev, Num, Deu, Dan, Joh, Rom, 1Co, 2Co,
Gal, Eph, Php, Col, 1Th, 2Th, 1Ti, 2Ti, Tit, Phm, Heb, Jas, 1Pe, 2Pe, 1Jo, 2Jo, 3Jo, Jud, Rev). The token "Dan"
often denotes the book Daniel and must NOT be interpreted as Indonesian "dan" ("and") when it appears as a book
abbreviation near a chapter/verse reference (e.g., "Dan 1:1"). Treat such abbreviations and references as language-
neutral signal.

Ambiguity rule: If the text is mixed or ambiguous, prefer English (en), especially when common English instruction
keywords are present (e.g., summarize, explain, what, who, why, how).

Output format: Return only structured output matching the schema { "language": <one of the allowed outputs> } with no
additional prose.

Disambiguation examples:
- text: "summarize Dan 1:1" -> { "language": "en" }
- text: "tolong ringkas Dan 1:1" -> { "language": "id" }
- text: "explain Joh 3:16" -> { "language": "en" }
- text: "ringkas Yoh 3:16" -> { "language": "id" }
"""

TARGET_TRANSLATION_LANGUAGE_AGENT_SYSTEM_PROMPT = """
Task: Determine the target language the user is asking the system to translate scripture into, based solely on the
user's latest message. Return an ISO 639-1 code from the allowed set.

Allowed outputs: en, ar, fr, es, hi, ru, id, sw, pt, zh, nl, Other

Rules:
- Identify explicit target-language mentions (language names, codes, or phrases like "into Russian", "to es",
  "in French").
- If no target language is explicitly specified, return Other. Do NOT infer a target from the message's language.
- Output must match the provided schema exactly with no extra prose.

Examples:
- message: "translate John 3:16 into Russian" -> { "language": "ru" }
- message: "please translate Mark 1 in Spanish" -> { "language": "es" }
- message: "translate Matthew 2" -> { "language": "Other" }
"""

BASE_DIR = Path(__file__).resolve().parent
DB_DIR = config.DATA_DIR

open_ai_client = OpenAI(api_key=config.OPENAI_API_KEY)

supported_language_map = {
    "en": "English",
    "ar": "Arabic",
    "fr": "French",
    "es": "Spanish",
    "hi": "Hindi",
    "ru": "Russian",
    "id": "Indonesian",
    "sw": "Swahili",
    "pt": "Portuguese",
    "zh": "Mandarin",
    "nl": "Dutch"
}

LANGUAGE_UNKNOWN = "UNKNOWN"

RELEVANCE_CUTOFF = .65
TOP_K = 5

logger = get_logger(__name__)


def _extract_cached_input_tokens(usage: Any) -> int | None:
    """Best-effort extraction of cached input token counts from SDK usage objects.

    Supports:
    - Responses API: usage.input_token_details.cache_read_input_tokens
    - Chat Completions: usage.prompt_tokens_details.cached_tokens
    Returns None when not available.
    """
    try:
        itd = getattr(usage, "input_token_details", None)
        if itd is not None:
            val = getattr(itd, "cache_read_input_tokens", None)
            if val is None and isinstance(itd, dict):
                val = itd.get("cache_read_input_tokens")
            if isinstance(val, int) and val > 0:
                return val
        ptd = getattr(usage, "prompt_tokens_details", None)
        if ptd is not None:
            val2 = getattr(ptd, "cached_tokens", None)
            if val2 is None and isinstance(ptd, dict):
                val2 = ptd.get("cached_tokens")
            if isinstance(val2, int) and val2 > 0:
                return val2
    except Exception:  # pylint: disable=broad-except
        return None
    return None


class Language(str, Enum):
    """Supported ISO 639-1 language codes for responses/messages."""
    ENGLISH = "en"
    ARABIC = "ar"
    FRENCH = "fr"
    SPANISH = "es"
    HINDI = "hi"
    RUSSIAN = "ru"
    INDONESIAN = "id"
    SWAHILI = "sw"
    PORTUGUESE = "pt"
    MANDARIN = "zh"
    DUTCH = "nl"
    OTHER = "Other"


class ResponseLanguage(BaseModel):
    """Model for parsing/validating the detected response language."""
    language: Language


SET_RESPONSE_LANGUAGE_AGENT_SYSTEM_PROMPT = """
Task: Determine the language the user wants responses in, based on conversation context and the latest message.

Allowed outputs: en, ar, fr, es, hi, ru, id, sw, pt, zh, nl, Other

Instructions:
- Use conversation history and the most recent message to infer the user's desired response language.
- Only return one of the allowed outputs. If unclear or unsupported, return Other.
- Consider explicit requests like "reply in French" or language names/codes.
- Output must match the provided schema with no additional prose.
"""


class MessageLanguage(BaseModel):
    """Model for parsing/validating the detected language of a message."""
    language: Language


class TranslatedPassage(BaseModel):
    """Schema for single-call passage translation output.

    - header_book: translated book name (e.g., "Иоанн").
    - header_suffix: exact suffix copied from input (e.g., "1:1–7").
    - body: translated passage body with original newlines preserved.
    - content_language: ISO 639-1 code (should equal requested target language).
    """

    header_book: str
    header_suffix: str
    body: str
    content_language: Language


class PreprocessorResult(BaseModel):
    """Result type for the preprocessor node output."""
    new_message: str
    reason_for_decision: str
    message_changed: bool


def _is_protected_response_item(item: dict) -> bool:
    """Return True if a response item carries scripture to protect from changes."""
    body = cast(dict | str, item.get("response"))
    if isinstance(body, dict):
        if body.get("suppress_translation"):
            return True
        if isinstance(body.get("segments"), list):
            segs = cast(list, body.get("segments"))
            return any(isinstance(seg, dict) and seg.get("type") == "scripture" for seg in segs)
    return False


def _reconstruct_structured_text(resp_item: dict | str, localize_to: Optional[str]) -> str:
    """Render a response item to plain text, optionally localizing the header book name.

    - If `resp_item` is a plain string, return it.
    - If structured with segments, rebuild: "<Book> <suffix>:\n\n<scripture>".
    - If `localize_to` is provided, map the book to that language via get_book_name; else use canonical.
    """
    if isinstance(resp_item, str):
        return resp_item
    body = cast(dict | str, resp_item.get("response"))
    if isinstance(body, dict) and isinstance(body.get("segments"), list):
        segs = cast(list, body.get("segments"))
        header_book = ""
        header_suffix = ""
        scripture_text = ""
        for seg in segs:
            if not isinstance(seg, dict):
                continue
            st = seg.get("type")
            txt = cast(str, seg.get("text", ""))
            if st == "header_book":
                header_book = txt
            elif st == "header_suffix":
                header_suffix = txt
            elif st == "scripture":
                scripture_text = txt
        book = get_book_name(localize_to or "en", header_book) if localize_to else header_book
        header = (f"{book} {header_suffix}" if header_suffix else book).strip() + ":"
        return header + ("\n\n" + scripture_text if scripture_text else "")
    return str(body)


class IntentType(str, Enum):
    """Enumeration of all supported user intents in the graph."""
    GET_BIBLE_TRANSLATION_ASSISTANCE = "get-bible-translation-assistance"
    GET_PASSAGE_SUMMARY = "get-passage-summary"
    GET_PASSAGE_KEYWORDS = "get-passage-keywords"
    GET_TRANSLATION_HELPS = "get-translation-helps"
    RETRIEVE_SCRIPTURE = "retrieve-scripture"
    TRANSLATE_SCRIPTURE = "translate-scripture"
    PERFORM_UNSUPPORTED_FUNCTION = "perform-unsupported-function"
    RETRIEVE_SYSTEM_INFORMATION = "retrieve-system-information"
    SET_RESPONSE_LANGUAGE = "set-response-language"
    CONVERSE_WITH_BT_SERVANT = 'converse-with-bt-servant'


class UserIntents(BaseModel):
    """Container for a list of user intents."""
    intents: List[IntentType]


class BrainState(TypedDict, total=False):
    """State carried through the LangGraph execution."""
    user_id: str
    user_query: str
    # Perf tracing: preserve trace id throughout the graph so node wrappers
    # can attach spans even when running in a thread pool.
    perf_trace_id: str
    query_language: str
    user_response_language: str
    transformed_query: str
    docs: List[Dict[str, str]]
    collection_used: str
    responses: Annotated[List[Dict[str, Any]], operator.add]
    translated_responses: List[str]
    stack_rank_collections: List[str]
    user_chat_history: List[Dict[str, str]]
    user_intents: List[IntentType]
    passage_selection: list[dict]


def start(state: Any) -> dict:
    """Handle first interaction greeting, otherwise no-op."""
    s = cast(BrainState, state)
    user_id = s["user_id"]
    if is_first_interaction(user_id):
        set_first_interaction(user_id, False)
        return {"responses": [
            {"intent": "first-interaction", "response": FIRST_INTERACTION_MESSAGE}]}
    return {}


def determine_intents(state: Any) -> dict:
    """Classify the user's transformed query into one or more intents."""
    s = cast(BrainState, state)
    query = s["transformed_query"]
    messages: list[EasyInputMessageParam] = [
        {
            "role": "system",
            "content": INTENT_CLASSIFICATION_AGENT_SYSTEM_PROMPT,
        },
        {
            "role": "user",
            "content": f"what is your classification of the latest user message: {query}",
        },
    ]
    response = open_ai_client.responses.parse(
        model="gpt-4o",
        input=cast(Any, messages),
        text_format=UserIntents,
        store=False
    )
    usage = getattr(response, "usage", None)
    if usage is not None:
        it = getattr(usage, "input_tokens", None)
        ot = getattr(usage, "output_tokens", None)
        tt = getattr(usage, "total_tokens", None)
        if tt is None and (it is not None or ot is not None):
            tt = (it or 0) + (ot or 0)
        cit = _extract_cached_input_tokens(usage)
        add_tokens(it, ot, tt, model="gpt-4o", cached_input_tokens=cit)
    user_intents_model = cast(UserIntents, response.output_parsed)
    logger.info("extracted user intents: %s", ' '.join([i.value for i in user_intents_model.intents]))

    return {
        "user_intents": user_intents_model.intents,
    }


class PassageRef(BaseModel):
    """Normalized reference to a passage within a single canonical book."""

    book: str
    start_chapter: int | None = None
    start_verse: int | None = None
    end_chapter: int | None = None
    end_verse: int | None = None


class PassageSelection(BaseModel):
    """Structured selection consisting of one or more ranges for a book."""

    selections: List[PassageRef]


def set_response_language(state: Any) -> dict:
    """Detect and persist the user's desired response language."""
    s = cast(BrainState, state)
    chat_input: list[EasyInputMessageParam] = [
        {
            "role": "user",
            "content": f"Past conversation: {json.dumps(s['user_chat_history'])}",
        },
        {
            "role": "user",
            "content": f"the user's most recent message: {s['user_query']}",
        },
        {
            "role": "user",
            "content": "What language is the user trying to set their response language to?",
        },
    ]
    response = open_ai_client.responses.parse(
        model="gpt-4o",
        instructions=SET_RESPONSE_LANGUAGE_AGENT_SYSTEM_PROMPT,
        input=cast(Any, chat_input),
        text_format=ResponseLanguage,
        temperature=0,
        store=False,
    )
    usage = getattr(response, "usage", None)
    if usage is not None:
        it = getattr(usage, "input_tokens", None)
        ot = getattr(usage, "output_tokens", None)
        tt = getattr(usage, "total_tokens", None)
        if tt is None and (it is not None or ot is not None):
            tt = (it or 0) + (ot or 0)
        cit = _extract_cached_input_tokens(usage)
        add_tokens(it, ot, tt, model="gpt-4o", cached_input_tokens=cit)
    resp_lang = cast(ResponseLanguage, response.output_parsed)
    if resp_lang.language == Language.OTHER:
        supported_language_list = ", ".join(supported_language_map.keys())
        response_text = (f"I think you're trying to set the response language. The supported languages "
                         f"are: {supported_language_list}. If this is your intent, please clearly tell "
                         f"me which supported language to use when responding.")
        return {"responses": [{"intent": IntentType.SET_RESPONSE_LANGUAGE, "response": response_text}]}
    user_id: str = s["user_id"]
    response_language_code: str = str(resp_lang.language.value)
    set_user_response_language(user_id, response_language_code)
    language_name: str = supported_language_map.get(response_language_code, response_language_code)
    response_text = f"Setting response language to: {language_name}"
    return {
        "responses": [{"intent": IntentType.SET_RESPONSE_LANGUAGE, "response": response_text}],
        "user_response_language": response_language_code
    }


def combine_responses(chat_history, latest_user_message, responses) -> str:
    """Ask OpenAI to synthesize multiple node responses into one coherent text.

    Note: callers should pass only normal responses here; scripture-protected
    items are excluded earlier to avoid rewriting scripture text.
    """
    uncombined_responses = json.dumps(responses)
    logger.info("preparing to combine responses:\n\n%s", uncombined_responses)
    messages: list[EasyInputMessageParam] = [
        {
            "role": "developer",
            "content": f"conversation history: {chat_history}",
        },
        {
            "role": "developer",
            "content": f"latest user message: {latest_user_message}",
        },
        {
            "role": "developer",
            "content": f"responses to synthesize: {uncombined_responses}",
        },
    ]
    response = open_ai_client.responses.create(
        model="gpt-4o",
        instructions=COMBINE_RESPONSES_SYSTEM_PROMPT,
        input=cast(Any, messages),
    )
    usage = getattr(response, "usage", None)
    if usage is not None:
        it = getattr(usage, "input_tokens", None)
        ot = getattr(usage, "output_tokens", None)
        tt = getattr(usage, "total_tokens", None)
        if tt is None and (it is not None or ot is not None):
            tt = (it or 0) + (ot or 0)
        cit = _extract_cached_input_tokens(usage)
        add_tokens(it, ot, tt, model="gpt-4o", cached_input_tokens=cit)
    combined = response.output_text
    logger.info("combined response from openai: %s", combined)
    return combined


def translate_responses(state: Any) -> dict:
    """Translate the response(s) into the user's desired language if needed.

    Scripture-protected responses are not combined via LLM and are not machine-
    translated. If present, they are passed through (with optional localized
    headers) and the remaining responses are combined and translated as needed.
    """
    s = cast(BrainState, state)
    uncombined = list(s["responses"])

    protected_items: list[dict] = [i for i in uncombined if _is_protected_response_item(i)]
    normal_items: list[dict] = [i for i in uncombined if not _is_protected_response_item(i)]

    responses_for_translation: list[dict | str] = list(protected_items)
    # Combine normal items (if any) using LLM synthesizer into a single string and append
    if normal_items:
        responses_for_translation.append(
            combine_responses(s["user_chat_history"], s["user_query"], normal_items)
        )
    if not responses_for_translation:
        raise ValueError("no responses to translate. something bad happened. bailing out.")

    if s["user_response_language"]:
        target_language = s["user_response_language"]
    else:
        target_language = s["query_language"]
        if target_language == LANGUAGE_UNKNOWN:
            logger.warning('target language unknown. bailing out.')
            # Build pass-through texts for current responses, then append notice
            passthrough_texts: list[str] = [
                _reconstruct_structured_text(resp_item=resp, localize_to=None)
                for resp in responses_for_translation
            ]
            passthrough_texts.append(
                "You haven't set your desired response language and I wasn't able to determine the language of your "
                "original message in order to match it. You can set your desired response language at any time by "
                "saying: Set my response language to Spanish, or Indonesian, or any of the supported languages: "
                f"{', '.join(supported_language_map.keys())}."
            )
            return {"translated_responses": passthrough_texts}

    translated_responses: list[str] = []
    for resp in responses_for_translation:
        if isinstance(resp, str):
            if detect_language(resp) != target_language:
                logger.info('preparing to translate to %s', target_language)
                translated_responses.append(translate_text(response_text=resp, target_language=target_language))
            else:
                logger.info('chunk translation not required. using chunk as is.')
                translated_responses.append(resp)
            continue

        # Structured/metadata response
        body = cast(dict | str, resp.get("response"))
        if isinstance(body, dict) and isinstance(body.get("segments"), list):
            item_lang = cast(Optional[str], body.get("content_language"))
            header_is_translated = bool(body.get("header_is_translated"))
            # If the header was already translated by the producer, do not localize again.
            # Otherwise, localize the canonical header to the passage's content language when known,
            # falling back to the target UI language.
            localize_to = None if header_is_translated else (item_lang or target_language)
            final_text2 = _reconstruct_structured_text(resp_item=resp, localize_to=localize_to)
            translated_responses.append(final_text2)
            continue

        # Unknown structured shape: fall back to string conversion
        translated_responses.append(str(body))
    return {
        "translated_responses": translated_responses
    }


def translate_text(response_text: str, target_language: str) -> str:
    """Translate a single text into the target ISO 639-1 language code.

    Returns a plain string. If the OpenAI SDK returns a structured content
    list or None, normalize it to a string.
    """
    chat_messages = cast(List[ChatCompletionMessageParam], [
        {
            "role": "system",
            "content": RESPONSE_TRANSLATOR_SYSTEM_PROMPT,
        },
        {
            "role": "user",
            "content": (
                f"text to translate: {response_text}\n\n"
                f"ISO 639-1 code representing target language: {target_language}"
            ),
        },
    ])
    completion = open_ai_client.chat.completions.create(
        model="gpt-4o",
        messages=chat_messages,
    )
    usage = getattr(completion, "usage", None)
    if usage is not None:
        it = getattr(usage, "prompt_tokens", None)
        ot = getattr(usage, "completion_tokens", None)
        tt = getattr(usage, "total_tokens", None)
        cit = _extract_cached_input_tokens(usage)
        add_tokens(it, ot, tt, model="gpt-4o", cached_input_tokens=cit)
    content = completion.choices[0].message.content
    if isinstance(content, list):
        text = "".join(part.get("text", "") if isinstance(part, dict) else "" for part in content)
    elif content is None:
        text = ""
    else:
        text = content
    logger.info('chunk: \n%s\n\ntranslated to:\n%s', response_text, text)
    return cast(str, text)


def detect_language(text) -> str:
    """Detect ISO 639-1 language code of the given text via OpenAI.

    Uses a domain-aware prompt with deterministic decoding and a light
    heuristic to avoid false Indonesian due to Bible abbreviations like
    "Dan" (Daniel).
    """
    messages: list[EasyInputMessageParam] = [
        {
            "role": "user",
            "content": f"text: {text}",
        },
    ]
    response = open_ai_client.responses.parse(
        model="gpt-4o",
        instructions=DETECT_LANGUAGE_AGENT_SYSTEM_PROMPT,
        input=cast(Any, messages),
        text_format=MessageLanguage,
        temperature=0,
        store=False,
    )
    usage = getattr(response, "usage", None)
    if usage is not None:
        it = getattr(usage, "input_tokens", None)
        ot = getattr(usage, "output_tokens", None)
        tt = getattr(usage, "total_tokens", None)
        if tt is None and (it is not None or ot is not None):
            tt = (it or 0) + (ot or 0)
        cit = _extract_cached_input_tokens(usage)
        add_tokens(it, ot, tt, model="gpt-4o", cached_input_tokens=cit)
    message_language = cast(MessageLanguage | None, response.output_parsed)
    predicted = message_language.language.value if message_language else "en"
    logger.info("language detection (model): %s", predicted)

    # Heuristic guard: If we predicted Indonesian ('id') but the text looks like
    # an English instruction paired with a Bible reference, prefer English.
    # This specifically addresses the common "Dan" (Daniel) vs Indonesian "dan" ambiguity.
    try:
        has_english_instruction = bool(
            re.search(r"\b(summarize|explain|what|who|why|how|list|give|provide)\b", str(text), re.IGNORECASE)
        )
        has_verse_pattern = bool(
            re.search(r"\b[A-Za-z]{2,4}\s+\d+:\d+\b", str(text))
        )
        logger.info(
            "heuristic_guard: predicted=%s english_instruction=%s verse_pattern=%s",
            predicted,
            has_english_instruction,
            has_verse_pattern,
        )
        if predicted == "id" and has_english_instruction and has_verse_pattern:
            logger.info("heuristic_guard: overriding id -> en due to English instruction + verse pattern")
            predicted = "en"
    except re.error as err:
        # If regex fails for any reason, fall back to the model prediction.
        logger.info("heuristic_guard: regex error (%s); keeping model prediction: %s", err, predicted)

    return predicted


def determine_query_language(state: Any) -> dict:
    """Determine the language of the user's original query and set collection order."""
    s = cast(BrainState, state)
    query = s["user_query"]
    query_language = detect_language(query)
    logger.info("language code %s detected by gpt-4o.", query_language)
    stack_rank_collections = [
        "knowledgebase",
        "en_resources",
    ]
    # If the detected language is not English, also search the matching
    # language-specific resources collection (e.g., "es_resources").
    if query_language and query_language != "en" and query_language != Language.OTHER.value:
        localized_collection = f"{query_language}_resources"
        stack_rank_collections.append(localized_collection)
        logger.info(
            "appended localized resources collection: %s (language=%s)",
            localized_collection,
            query_language,
        )

    return {
        "query_language": query_language,
        "stack_rank_collections": stack_rank_collections
    }


def preprocess_user_query(state: Any) -> dict:  # pylint: disable=too-many-locals
    """Lightly clarify or correct the user's query using conversation history."""
    s = cast(BrainState, state)
    query = s["user_query"]
    chat_history = s["user_chat_history"]
    history_context_message = f"past_conversation: {json.dumps(chat_history)}"
    messages: list[EasyInputMessageParam] = [
        {
            "role": "user",
            "content": history_context_message,
        },
        {
            "role": "user",
            "content": f"current_message: {query}",
        },
    ]
    response = open_ai_client.responses.parse(
        model="gpt-4o",
        instructions=PREPROCESSOR_AGENT_SYSTEM_PROMPT,
        input=cast(Any, messages),
        text_format=PreprocessorResult,
        store=False
    )
    usage = getattr(response, "usage", None)
    if usage is not None:
        it = getattr(usage, "input_tokens", None)
        ot = getattr(usage, "output_tokens", None)
        tt = getattr(usage, "total_tokens", None)
        if tt is None and (it is not None or ot is not None):
            tt = (it or 0) + (ot or 0)
        cit = _extract_cached_input_tokens(usage)
        add_tokens(it, ot, tt, model="gpt-4o", cached_input_tokens=cit)
    preprocessor_result = cast(PreprocessorResult | None, response.output_parsed)
    if preprocessor_result is None:
        new_message = query
        reason_for_decision = "no changes"
        message_changed = False
    else:
        new_message = preprocessor_result.new_message
        reason_for_decision = preprocessor_result.reason_for_decision
        message_changed = preprocessor_result.message_changed
    logger.info("new_message: %s\nreason_for_decision: %s\nmessage_changed: %s",
                new_message, reason_for_decision, message_changed)
    return {
        "transformed_query": new_message if message_changed else query
    }


def query_vector_db(state: Any) -> dict:
    """Query the vector DB (Chroma) across ranked collections and filter by relevance."""
    # pylint: disable=too-many-locals
    s = cast(BrainState, state)
    query = s["transformed_query"]
    stack_rank_collections = s["stack_rank_collections"]
    filtered_docs = []
    # this loop is the current implementation of the "stacked ranked" algorithm
    for collection_name in stack_rank_collections:
        logger.info("querying stack collection: %s", collection_name)
        db_collection = get_chroma_collection(collection_name)
        if not db_collection:
            logger.warning("collection %s was not found in chroma db.", collection_name)
            continue
        col = cast(Any, db_collection)
        results = col.query(
            query_texts=[query],
            n_results=TOP_K
        )
        docs = results["documents"]
        similarities = results["distances"]
        metadata = results["metadatas"]
        logger.info("\nquery: %s\n", query)
        logger.info("---")
        hits = 0
        for i in range(len(docs[0])):
            cosine_similarity = round(1 - similarities[0][i], 4)
            doc = docs[0][i]
            m = metadata[0][i]
            resource_name = m.get("name", "")
            source = m.get("source", "")
            logger.info("processing %s from %s.", resource_name, source)
            logger.info("Cosine Similarity: %s", cosine_similarity)
            logger.info("Metadata: %s", resource_name)
            logger.info("---")
            if cosine_similarity >= RELEVANCE_CUTOFF:
                hits += 1
                filtered_docs.append({
                    "collection_name": collection_name,
                    "resource_name": resource_name,
                    "source": source,
                    "document_text": doc
                })
        if hits > 0:
            logger.info("found %d hit(s) at stack collection: %s", hits, collection_name)

    return {
        "docs": filtered_docs
    }


# pylint: disable=too-many-locals
def query_open_ai(state: Any) -> dict:
    """Generate the final response text using RAG context and OpenAI."""
    s = cast(BrainState, state)
    docs = s["docs"]
    query = s["transformed_query"]
    chat_history = s["user_chat_history"]
    try:
        if len(docs) == 0:
            no_docs_msg = (f"Sorry, I couldn't find any information in my resources to service your request "
                           f"or command.\n\n{BOILER_PLATE_AVAILABLE_FEATURES_MESSAGE}")
            return {"responses": [
                {"intent": IntentType.GET_BIBLE_TRANSLATION_ASSISTANCE, "response": no_docs_msg}]}

        # build context from docs
        # context = "\n\n".join([item["doc"] for item in docs])
        context = json.dumps(docs, indent=2)
        logger.info("context passed to final node:\n\n%s", context)
        rag_context_message = "When answering my next query, use this additional" + \
            f"  context: {context}"
        chat_history_context_message = (f"Use this conversation history to understand the user's "
                                        f"current request only if needed: {json.dumps(chat_history)}")
        messages = cast(List[EasyInputMessageParam], [
            {
                "role": "developer",
                "content": rag_context_message
            },
            {
                "role": "developer",
                "content": chat_history_context_message
            },
            {
                "role": "user",
                "content": query
            }
        ])
        response = open_ai_client.responses.create(
            model="gpt-4o",
            instructions=FINAL_RESPONSE_AGENT_SYSTEM_PROMPT,
            input=cast(Any, messages)
        )
        usage = getattr(response, "usage", None)
        if usage is not None:
            it = getattr(usage, "input_tokens", None)
            ot = getattr(usage, "output_tokens", None)
            tt = getattr(usage, "total_tokens", None)
            if tt is None and (it is not None or ot is not None):
                tt = (it or 0) + (ot or 0)
            cit = _extract_cached_input_tokens(usage)
            add_tokens(it, ot, tt, model="gpt-4o", cached_input_tokens=cit)
        bt_servant_response = response.output_text
        logger.info('response from openai: %s', bt_servant_response)
        logger.debug("%d characters returned from openAI", len(bt_servant_response))

        resource_list = ", ".join({
            f"{item.get('resource_name', 'unknown')} from {item.get('source', 'unknown')}"
            for item in docs
        })
        cascade_info = (
            f"bt servant used the following resources to generate its response: {resource_list}."
        )
        logger.info(cascade_info)

        return {"responses": [
            {"intent": IntentType.GET_BIBLE_TRANSLATION_ASSISTANCE, "response": bt_servant_response}]}
    except OpenAIError:
        logger.error("Error during OpenAI request", exc_info=True)
        error_msg = "I encountered some problems while trying to respond. Let Ian know about this one."
        return {"responses": [{"intent": IntentType.GET_BIBLE_TRANSLATION_ASSISTANCE, "response": error_msg}]}


def chunk_message(state: Any) -> dict:
    """Chunk oversized responses to respect WhatsApp limits, via LLM or fallback."""
    logger.info("MESSAGE TOO BIG. CHUNKING...")
    s = cast(BrainState, state)
    responses = s["translated_responses"]
    text_to_chunk = responses[0]
    chunk_max = config.MAX_META_TEXT_LENGTH - 100
    try:
        chat_messages = cast(List[ChatCompletionMessageParam], [
            {
                "role": "system",
                "content": CHOP_AGENT_SYSTEM_PROMPT,
            },
            {
                "role": "user",
                "content": f"text to chop: \n\n{text_to_chunk}",
            },
        ])
        completion = open_ai_client.chat.completions.create(
            model='gpt-4o',
            messages=chat_messages,
        )
        usage = getattr(completion, "usage", None)
        if usage is not None:
            it = getattr(usage, "prompt_tokens", None)
            ot = getattr(usage, "completion_tokens", None)
            tt = getattr(usage, "total_tokens", None)
            cit = _extract_cached_input_tokens(usage)
            add_tokens(it, ot, tt, model="gpt-4o", cached_input_tokens=cit)
        response_content = completion.choices[0].message.content
        if not isinstance(response_content, str):
            raise ValueError("empty or non-text content from chat completion")
        chunks = json.loads(response_content)
    except (OpenAIError, json.JSONDecodeError, ValueError):
        logger.error("LLM chunking failed. Falling back to deterministic chunking.", exc_info=True)
        chunks = None

    # Deterministic safeguards: if LLM returned a single massive chunk or invalid shape,
    # or if we skipped to fallback
    def _pack_items(items: list[str], max_len: int) -> list[str]:
        out: list[str] = []
        cur = ""
        for it in items:
            sep = (", " if cur else "")
            if len(cur) + len(sep) + len(it) <= max_len:
                cur += sep + it
            else:
                if cur:
                    out.append(cur)
                if len(it) <= max_len:
                    cur = it
                else:
                    # hard-split this long token
                    for j in range(0, len(it), max_len):
                        out.append(it[j:j+max_len])
                    cur = ""
        if cur:
            out.append(cur)
        return out

    if not isinstance(chunks, list) or any(not isinstance(c, str) for c in chunks):
        # Try delimiter-aware fallback for comma-heavy lists first
        if text_to_chunk.count(",") >= 10:
            parts = [p.strip() for p in text_to_chunk.split(",") if p.strip()]
            chunks = _pack_items(parts, chunk_max)
        else:
            chunks = chop_text(text=text_to_chunk, n=chunk_max)
    else:
        # Ensure each chunk respects the limit; if not, re-split deterministically
        fixed: list[str] = []
        for c in chunks:
            if len(c) <= chunk_max:
                fixed.append(c)
            else:
                if c.count(",") >= 10:
                    parts = [p.strip() for p in c.split(",") if p.strip()]
                    fixed.extend(_pack_items(parts, chunk_max))
                else:
                    fixed.extend(chop_text(text=c, n=chunk_max))
        chunks = fixed

    chunks.extend(responses[1:])
    return {"translated_responses": combine_chunks(chunks=chunks, chunk_max=chunk_max)}


def needs_chunking(state: BrainState) -> str:
    """Return next node key if chunking is required, otherwise finish."""
    first_response = state["translated_responses"][0]
    if len(first_response) > config.MAX_META_TEXT_LENGTH:
        logger.warning('message to big: %d chars. preparing to chunk.', len(first_response))
        return "chunk_message_node"
    return END


def process_intents(state: Any) -> List[Hashable]:
    """Map detected intents to the list of nodes to traverse."""
    s = cast(BrainState, state)
    user_intents = s["user_intents"]
    if not user_intents:
        raise ValueError("no intents found. something went very wrong.")

    nodes_to_traverse: List[Hashable] = []
    if IntentType.GET_BIBLE_TRANSLATION_ASSISTANCE in user_intents:
        nodes_to_traverse.append("query_vector_db_node")
    if IntentType.GET_PASSAGE_SUMMARY in user_intents:
        nodes_to_traverse.append("handle_get_passage_summary_node")
    if IntentType.GET_PASSAGE_KEYWORDS in user_intents:
        nodes_to_traverse.append("handle_get_passage_keywords_node")
    if IntentType.GET_TRANSLATION_HELPS in user_intents:
        nodes_to_traverse.append("handle_get_translation_helps_node")
    if IntentType.RETRIEVE_SCRIPTURE in user_intents:
        nodes_to_traverse.append("handle_retrieve_scripture_node")
    if IntentType.SET_RESPONSE_LANGUAGE in user_intents:
        nodes_to_traverse.append("set_response_language_node")
    if IntentType.PERFORM_UNSUPPORTED_FUNCTION in user_intents:
        nodes_to_traverse.append("handle_unsupported_function_node")
    if IntentType.RETRIEVE_SYSTEM_INFORMATION in user_intents:
        nodes_to_traverse.append("handle_system_information_request_node")
    if IntentType.CONVERSE_WITH_BT_SERVANT in user_intents:
        nodes_to_traverse.append("converse_with_bt_servant_node")
    if IntentType.RETRIEVE_SCRIPTURE in user_intents:
        nodes_to_traverse.append("handle_retrieve_scripture_node")
    if IntentType.TRANSLATE_SCRIPTURE in user_intents:
        nodes_to_traverse.append("handle_translate_scripture_node")

    return nodes_to_traverse


def handle_unsupported_function(state: Any) -> dict:
    """Generate a helpful response when the user requests unsupported functionality."""
    s = cast(BrainState, state)
    query = s["user_query"]
    chat_history = s["user_chat_history"]
    messages: list[EasyInputMessageParam] = [
        {
            "role": "developer",
            "content": f"Conversation history to use if needed: {json.dumps(chat_history)}",
        },
        {
            "role": "user",
            "content": query,
        },
    ]
    response = open_ai_client.responses.create(
        model="gpt-4o",
        instructions=UNSUPPORTED_FUNCTION_AGENT_SYSTEM_PROMPT,
        input=cast(Any, messages),
        store=False
    )
    usage = getattr(response, "usage", None)
    if usage is not None:
        it = getattr(usage, "input_tokens", None)
        ot = getattr(usage, "output_tokens", None)
        tt = getattr(usage, "total_tokens", None)
        if tt is None and (it is not None or ot is not None):
            tt = (it or 0) + (ot or 0)
        cit = _extract_cached_input_tokens(usage)
        add_tokens(it, ot, tt, model="gpt-4o", cached_input_tokens=cit)
    unsupported_function_response_text = response.output_text
    logger.info('converse_with_bt_servant response from openai: %s', unsupported_function_response_text)
    return {"responses": [{"intent": IntentType.PERFORM_UNSUPPORTED_FUNCTION, "response": unsupported_function_response_text}]}


def handle_system_information_request(state: Any) -> dict:
    """Provide help/about information for the BT Servant system."""
    s = cast(BrainState, state)
    query = s["user_query"]
    chat_history = s["user_chat_history"]
    messages: list[EasyInputMessageParam] = [
        {
            "role": "developer",
            "content": f"Conversation history to use if needed: {json.dumps(chat_history)}",
        },
        {
            "role": "user",
            "content": query,
        },
    ]
    response = open_ai_client.responses.create(
        model="gpt-4o",
        instructions=HELP_AGENT_SYSTEM_PROMPT,
        input=cast(Any, messages),
        store=False
    )
    usage = getattr(response, "usage", None)
    if usage is not None:
        it = getattr(usage, "input_tokens", None)
        ot = getattr(usage, "output_tokens", None)
        tt = getattr(usage, "total_tokens", None)
        if tt is None and (it is not None or ot is not None):
            tt = (it or 0) + (ot or 0)
        cit = _extract_cached_input_tokens(usage)
        add_tokens(it, ot, tt, model="gpt-4o", cached_input_tokens=cit)
    help_response_text = response.output_text
    logger.info('help response from openai: %s', help_response_text)
    return {"responses": [{"intent": IntentType.RETRIEVE_SYSTEM_INFORMATION, "response": help_response_text}]}


def converse_with_bt_servant(state: Any) -> dict:
    """Respond conversationally to the user based on context and history."""
    s = cast(BrainState, state)
    query = s["user_query"]
    chat_history = s["user_chat_history"]
    messages: list[EasyInputMessageParam] = [
        {
            "role": "developer",
            "content": f"Conversation history to use if needed: {json.dumps(chat_history)}",
        },
        {
            "role": "user",
            "content": query,
        },
    ]
    response = open_ai_client.responses.create(
        model="gpt-4o",
        instructions=CONVERSE_AGENT_SYSTEM_PROMPT,
        input=cast(Any, messages),
        store=False
    )
    usage = getattr(response, "usage", None)
    if usage is not None:
        it = getattr(usage, "input_tokens", None)
        ot = getattr(usage, "output_tokens", None)
        tt = getattr(usage, "total_tokens", None)
        if tt is None and (it is not None or ot is not None):
            tt = (it or 0) + (ot or 0)
        cit = _extract_cached_input_tokens(usage)
        add_tokens(it, ot, tt, model="gpt-4o", cached_input_tokens=cit)
    converse_response_text = response.output_text
    logger.info('converse_with_bt_servant response from openai: %s', converse_response_text)
    return {"responses": [{"intent": IntentType.CONVERSE_WITH_BT_SERVANT, "response": converse_response_text}]}


def _book_patterns() -> list[tuple[str, str]]:
    """Return (canonical, regex) patterns to detect book mentions (ordered)."""
    pats: list[tuple[str, str]] = []
    for canonical, meta in BSB_BOOK_MAP.items():
        # canonical name as a whole word/phrase
        cn = re.escape(canonical)
        pats.append((canonical, rf"\b{cn}\b"))
        # short ref abbreviation (e.g., Gen, Exo, 1Sa)
        abbr = re.escape(meta.get("ref_abbr", ""))
        if abbr:
            pats.append((canonical, rf"\b{abbr}\b"))
    return pats


def _detect_mentioned_books(text: str) -> list[str]:
    """Detect canonical books mentioned in text, preserving order of appearance."""
    found: list[tuple[int, str]] = []
    lower = text
    for canonical, pattern in _book_patterns():
        for m in re.finditer(pattern, lower, flags=re.IGNORECASE):
            found.append((m.start(), canonical))
    # sort by appearance and dedupe preserving order
    found.sort(key=lambda t: t[0])
    seen = set()
    ordered: list[str] = []
    for _, can in found:
        if can not in seen:
            seen.add(can)
            ordered.append(can)
    return ordered


def _choose_primary_book(text: str, candidates: list[str]) -> str | None:
    """Heuristic to pick a primary book when multiple are mentioned.

    Prefer the first mentioned that appears near chapter/verse digits; else None.
    """
    if not candidates:
        return None
    # Build spans for each candidate occurrence
    spans: list[tuple[int, int, str]] = []
    for can, pat in _book_patterns():
        if can not in candidates:
            continue
        for m in re.finditer(pat, text, flags=re.IGNORECASE):
            spans.append((m.start(), m.end(), can))
    spans.sort(key=lambda t: t[0])
    for s_idx, end, can in spans:
        _ = s_idx  # avoid shadowing outer start() function
        window = text[end:end + 12]
        if re.search(r"\d", window):
            return can
    return None


def _resolve_selection_for_single_book(
    query: str,
    query_lang: str,
) -> tuple[str | None, list[tuple[int, int | None, int | None, int | None]] | None, str | None]:
    # pylint: disable=too-many-return-statements, too-many-branches
    """Parse and normalize a user query into a single canonical book and ranges.

    Returns a tuple of (canonical_book, ranges, error_message). On success, the
    error_message is None. On failure, canonical_book and ranges are None and
    error_message contains a user-friendly explanation.
    """
    logger.info("[selection-helper] start; query_lang=%s; query=%s", query_lang, query)

    # Translate to English for parsing, if needed
    if query_lang == Language.ENGLISH.value:
        parse_input = query
        logger.info("[selection-helper] parsing in English (no translation needed)")
    else:
        logger.info("[selection-helper] translating query to English for parsing")
        parse_input = translate_text(query, target_language="en")

    # Build selection prompt with canonical books
    books = ", ".join(BSB_BOOK_MAP.keys())
    system_prompt = PASSAGE_SELECTION_AGENT_SYSTEM_PROMPT.format(books=books)
    selection_messages: list[EasyInputMessageParam] = cast(List[EasyInputMessageParam], [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": parse_input},
    ])
    logger.info("[selection-helper] extracting passage selection via LLM")
    selection_resp = open_ai_client.responses.parse(
        model="gpt-4o",
        input=cast(Any, selection_messages),
        text_format=PassageSelection,
        store=False,
    )
    usage = getattr(selection_resp, "usage", None)
    if usage is not None:
        it = getattr(usage, "input_tokens", None)
        ot = getattr(usage, "output_tokens", None)
        tt = getattr(usage, "total_tokens", None)
        if tt is None and (it is not None or ot is not None):
            tt = (it or 0) + (ot or 0)
        cit = _extract_cached_input_tokens(usage)
        add_tokens(it, ot, tt, model="gpt-4o", cached_input_tokens=cit)
    selection = cast(PassageSelection, selection_resp.output_parsed)
    logger.info("[selection-helper] extracted %d selection(s)", len(selection.selections))

    # Detect books explicitly mentioned in the user input for cross-book guardrails
    mentioned = _detect_mentioned_books(parse_input)

    # Heuristic correction for explicit "chapters X–Y"
    lower_in = parse_input.lower()
    chap_match = re.search(r"\bchapters?\s+(\d+)\s*[-–]\s*(\d+)\b", lower_in)
    if chap_match and selection.selections:
        a, b = int(chap_match.group(1)), int(chap_match.group(2))
        logger.info("[selection-helper] correcting to multi-chapter range: %d-%d due to 'chapters' phrasing", a, b)
        first = selection.selections[0]
        selection.selections[0] = PassageRef(
            book=first.book,
            start_chapter=a,
            start_verse=None,
            end_chapter=b,
            end_verse=None,
        )

    if not selection.selections:
        # If multiple books are clearly mentioned, prefer consistent cross-book guidance,
        # but allow a tie-break fallback when one has explicit digits nearby.
        if len(mentioned) >= 2:
            msg = (
                "Please request a selection for one book at a time. "
                "If you need multiple books, send a separate message for each."
            )
            logger.info("[selection-helper] empty parse; multiple books detected -> cross-book message")
            return None, None, msg
        if len(mentioned) == 1:
            # Fallback: choose the single detected book as a whole-book selection.
            primary = mentioned[0]
            logger.info("[selection-helper] empty parse; falling back to single detected book: %s", primary)
            return primary, [(1, None, 10_000, None)], None
        msg = (
            "I couldn't identify a clear Bible passage in your request. Supported selection types include: "
            "single verse (e.g., John 3:16); verse range within a chapter (John 3:16-18); cross-chapter within a "
            "single book (John 3:16–4:2); whole chapter (John 3); multi-chapter span with no verses (John 1–4); "
            "or the whole book (John). Multiple books in one request are not supported — please choose one book."
        )
        logger.info("[selection-helper] no passage detected; returning guidance message")
        return None, None, msg

    # Note: Do not veto a successful single-book selection based on mentions.
    # Mentions are only used for empty-parse fallback above.

    # Ensure all selections are within the same canonical book and normalize
    canonical_books: list[str] = []
    normalized_selections: list[PassageRef] = []
    for sel in selection.selections:
        canonical = normalize_book_name(sel.book) or sel.book
        if canonical not in BSB_BOOK_MAP:
            supported = ", ".join(BSB_BOOK_MAP.keys())
            msg = (
                f"The book '{sel.book}' is not recognized. Please use a supported canonical book name. "
                f"Supported books include: {supported}."
            )
            logger.info("[selection-helper] unsupported book requested: %s", sel.book)
            return None, None, msg
        canonical_books.append(canonical)
        normalized_selections.append(PassageRef(
            book=canonical,
            start_chapter=sel.start_chapter,
            start_verse=sel.start_verse,
            end_chapter=sel.end_chapter,
            end_verse=sel.end_verse,
        ))

    if len(set(canonical_books)) != 1:
        msg = (
            "Please request a selection for one book at a time. "
            "If you need multiple books, send a separate message for each."
        )
        logger.info("[selection-helper] cross-book selection detected")
        return None, None, msg

    canonical_book = canonical_books[0]
    logger.info("[selection-helper] canonical_book=%s", canonical_book)

    # Build ranges: if no chapter info, interpret as whole book
    ranges: list[tuple[int, int | None, int | None, int | None]] = []
    for sel in normalized_selections:
        if sel.start_chapter is None:
            ranges.append((1, None, 10_000, None))
        else:
            ranges.append((sel.start_chapter, sel.start_verse, sel.end_chapter, sel.end_verse))

    logger.info("[selection-helper] ranges=%s", ranges)
    return canonical_book, ranges, None


def handle_get_passage_summary(state: Any) -> dict:
    """Handle get-passage-summary: extract refs, retrieve verses, summarize.

    - If user query language is not English, translate the transformed query to English
      for extraction only.
    - Extract passage selection via structured LLM parse with a strict prompt and
      canonical book list.
    - Validate constraints (single book, up to whole book; no cross-book).
    - Load verses from sources/bible_data/en efficiently and summarize.
    - Return a single combined summary prefixed with a canonical reference echo.
    """
    s = cast(BrainState, state)
    query = s["transformed_query"]
    query_lang = s["query_language"]
    logger.info("[passage-summary] start; query_lang=%s; query=%s", query_lang, query)

    canonical_book, ranges, err = _resolve_selection_for_single_book(query, query_lang)
    if err:
        return {"responses": [{"intent": IntentType.GET_PASSAGE_SUMMARY, "response": err}]}
    assert canonical_book is not None and ranges is not None

    # Retrieve verses from BSB JSONs; data dir is project root / sources/bible_data/en/bsb
    data_root = Path("sources") / "bible_data" / "en" / "bsb"
    logger.info("[passage-summary] retrieving verses from %s", data_root)
    verses = select_verses(data_root, canonical_book, ranges)
    logger.info("[passage-summary] retrieved %d verse(s)", len(verses))
    if not verses:
        msg = (
            "I couldn't locate those verses in the BSB data. Please check the reference and try again."
        )
        logger.info("[passage-summary] no verses found for selection; prompting user")
        return {"responses": [{"intent": IntentType.GET_PASSAGE_SUMMARY, "response": msg}]}

    # Prepare text for summarization
    ref_label = label_ranges(canonical_book, ranges)
    logger.info("[passage-summary] label=%s", ref_label)
    joined = "\n".join(f"{ref}: {txt}" for ref, txt in verses)

    # Summarize using LLM with strict system prompt
    sum_messages: list[EasyInputMessageParam] = [
        {"role": "developer", "content": f"Passage reference: {ref_label}"},
        {"role": "developer", "content": f"Passage verses (use only this content):\n{joined}"},
        {"role": "user", "content": "Provide a concise, faithful summary of the passage above."},
    ]
    logger.info("[passage-summary] summarizing %d verses", len(verses))
    summary_resp = open_ai_client.responses.create(
        model="gpt-4o",
        instructions=PASSAGE_SUMMARY_AGENT_SYSTEM_PROMPT,
        input=cast(Any, sum_messages),
        store=False,
    )
    usage = getattr(summary_resp, "usage", None)
    if usage is not None:
        it = getattr(usage, "input_tokens", None)
        ot = getattr(usage, "output_tokens", None)
        tt = getattr(usage, "total_tokens", None)
        if tt is None and (it is not None or ot is not None):
            tt = (it or 0) + (ot or 0)
        cit = _extract_cached_input_tokens(usage)
        add_tokens(it, ot, tt, model="gpt-4o", cached_input_tokens=cit)
    summary_text = summary_resp.output_text
    logger.info("[passage-summary] summary generated (len=%d)", len(summary_text) if summary_text else 0)

    response_text = f"Summary of {ref_label}:\n\n{summary_text}"
    logger.info("[passage-summary] done")
    return {"responses": [{"intent": IntentType.GET_PASSAGE_SUMMARY, "response": response_text}]}


def handle_get_passage_keywords(state: Any) -> dict:
    """Handle get-passage-keywords: extract refs, retrieve keywords, and list them.

    Mirrors the summary flow for selection parsing and validation, but instead of
    summarizing verses, loads per-verse keyword data from sources/keyword_data and
    returns a comma-separated list of distinct tw_match values present in the
    selection. The response is prefixed with "Keywords in <range>\n\n".
    """
    s = cast(BrainState, state)
    query = s["transformed_query"]
    query_lang = s["query_language"]
    logger.info("[passage-keywords] start; query_lang=%s; query=%s", query_lang, query)

    canonical_book, ranges, err = _resolve_selection_for_single_book(query, query_lang)
    if err:
        return {"responses": [{"intent": IntentType.GET_PASSAGE_KEYWORDS, "response": err}]}
    assert canonical_book is not None and ranges is not None

    # Retrieve keywords from keyword dataset
    data_root = Path("sources") / "keyword_data"
    logger.info("[passage-keywords] retrieving keywords from %s", data_root)
    keywords = select_keywords(data_root, canonical_book, ranges)
    logger.info("[passage-keywords] retrieved %d keyword(s)", len(keywords))

    if not keywords:
        msg = (
            "I couldn't locate keywords for that selection. Please check the reference and try again."
        )
        logger.info("[passage-keywords] no keywords found; prompting user")
        return {"responses": [{"intent": IntentType.GET_PASSAGE_KEYWORDS, "response": msg}]}

    ref_label = label_ranges(canonical_book, ranges)
    header = f"Keywords in {ref_label}\n\n"
    body = ", ".join(keywords)
    response_text = header + body
    logger.info("[passage-keywords] done")
    return {"responses": [{"intent": IntentType.GET_PASSAGE_KEYWORDS, "response": response_text}]}


TRANSLATION_HELPS_AGENT_SYSTEM_PROMPT = """
# Identity

You are a careful assistant helping Bible translators anticipate and address translation issues.

# Instructions

You will receive a structured JSON context containing:
- selection metadata (book and ranges),
- per-verse translation helps (with BSB/ULT verse text and notes).

Use only the provided context to write a coherent, actionable guide for translators. Focus on:
- key translation issues surfaced by the notes,
- clarifications about original-language expressions noted in the helps,
- concrete guidance and options for difficult terms, and
- any cross-references or constraints hinted by support references.

Style:
- Write in clear prose (avoid lists unless the content is inherently a short list).
- Cite verse numbers inline (e.g., “1:1–3”, “3:16”) where helpful.
- Be faithful and restrained; do not speculate beyond the provided context.
"""


def handle_get_translation_helps(state: Any) -> dict:
    """Handle get-translation-helps: extract refs, load helps, and guide.

    - Parse and validate a single-book selection via the shared helper.
    - Load per-verse translation helps from sources/translation_helps.
    - Provide a structured JSON context to the LLM and return a guidance response.
    """
    s = cast(BrainState, state)
    query = s["transformed_query"]
    query_lang = s["query_language"]
    logger.info("[translation-helps] start; query_lang=%s; query=%s", query_lang, query)

    canonical_book, ranges, err = _resolve_selection_for_single_book(query, query_lang)
    if err:
        return {"responses": [{"intent": IntentType.GET_TRANSLATION_HELPS, "response": err}]}
    assert canonical_book is not None and ranges is not None

    th_root = Path("sources") / "translation_helps"
    logger.info("[translation-helps] loading helps from %s", th_root)
    # Special-case: book entirely missing from translation helps dataset
    missing_books = set(get_missing_th_books(th_root))
    if canonical_book in missing_books:
        abbrs = sorted(BSB_BOOK_MAP[b]["ref_abbr"] for b in missing_books)
        requested_abbr = BSB_BOOK_MAP[canonical_book]["ref_abbr"]
        msg = (
            f"Translation helps for {requested_abbr} are not available yet. "
            f"Currently missing books: {', '.join(abbrs)}. "
            "Would you like translation help for one of the supported books instead?"
        )
        return {"responses": [{"intent": IntentType.GET_TRANSLATION_HELPS, "response": msg}]}
    # Count total verses first; if above limit, return a user-facing error instead of truncating
    bsb_root = Path("sources") / "bible_data" / "en" / "bsb"
    verse_count = len(select_verses(bsb_root, canonical_book, ranges))
    if verse_count > config.TRANSLATION_HELPS_VERSE_LIMIT:
        ref_label_over = label_ranges(canonical_book, ranges)
        msg = (
            f"I can only provide translate help for {config.TRANSLATION_HELPS_VERSE_LIMIT} verses at a time. "
            f"Your selection {ref_label_over} includes {verse_count} verses. Please narrow the range (e.g., a chapter or a shorter span)."
        )
        return {"responses": [{"intent": IntentType.GET_TRANSLATION_HELPS, "response": msg}]}
    # Enforce verse-count limit to control context/token size
    limited_ranges = clamp_ranges_by_verse_limit(
        bsb_root,
        canonical_book,
        ranges,
        max_verses=config.TRANSLATION_HELPS_VERSE_LIMIT,
    )
    if not limited_ranges:
        msg = "I couldn't identify verses for that selection in the BSB index. Please try another reference."
        return {"responses": [{"intent": IntentType.GET_TRANSLATION_HELPS, "response": msg}]}
    helps = select_translation_helps(th_root, canonical_book, limited_ranges)
    logger.info("[translation-helps] selected %d help entries", len(helps))
    if not helps:
        msg = (
            "I couldn't locate translation helps for that selection. Please check the reference and try again."
        )
        return {"responses": [{"intent": IntentType.GET_TRANSLATION_HELPS, "response": msg}]}

    ref_label = label_ranges(canonical_book, limited_ranges)
    context_obj = {
        "reference_label": ref_label,
        "selection": {
            "book": canonical_book,
            "ranges": [
                {
                    "start_chapter": sc,
                    "start_verse": sv,
                    "end_chapter": ec,
                    "end_verse": ev,
                }
                for (sc, sv, ec, ev) in limited_ranges
            ],
        },
        "translation_helps": helps,
    }

    messages: list[EasyInputMessageParam] = [
        {"role": "developer", "content": f"Selection: {ref_label}"},
        {"role": "developer", "content": "Use the JSON context below strictly:"},
        {"role": "developer", "content": json.dumps(context_obj, ensure_ascii=False)},
        {"role": "user", "content": "Using the provided context, explain the translation challenges and give actionable guidance for this selection."},
    ]

    logger.info("[translation-helps] invoking LLM with %d helps", len(helps))
    resp = open_ai_client.responses.create(
        model="gpt-4o",
        instructions=TRANSLATION_HELPS_AGENT_SYSTEM_PROMPT,
        input=cast(Any, messages),
        store=False,
    )
    usage = getattr(resp, "usage", None)
    if usage is not None:
        it = getattr(usage, "input_tokens", None)
        ot = getattr(usage, "output_tokens", None)
        tt = getattr(usage, "total_tokens", None)
        if tt is None and (it is not None or ot is not None):
            tt = (it or 0) + (ot or 0)
        cit = _extract_cached_input_tokens(usage)
        add_tokens(it, ot, tt, model="gpt-4o", cached_input_tokens=cit)
    text = resp.output_text
    header = f"Translation helps for {ref_label}\n\n"
    response_text = header + (text or "")
    return {"responses": [{"intent": IntentType.GET_TRANSLATION_HELPS, "response": response_text}]}


def handle_retrieve_scripture(state: Any) -> dict:
    """Handle retrieve-scripture: extract refs and return exact verse text.

    - Resolve selection via shared helper (single canonical book; ranges allowed).
    - Resolve data root via response_language → query_language → en, or explicit
      language mention in the user query (e.g., "Indonesian Bible").
    - Build a structured response with segments so downstream translation logic
      can localize only the header while leaving scripture untouched.
    """
    s = cast(BrainState, state)
    query = s["transformed_query"]
    query_lang = s["query_language"]
    logger.info("[retrieve-scripture] start; query_lang=%s; query=%s", query_lang, query)

    # 1) Parse passage selection
    canonical_book, ranges, err = _resolve_selection_for_single_book(query, query_lang)
    if err:
        return {"responses": [{"intent": IntentType.RETRIEVE_SCRIPTURE, "response": err}]}
    assert canonical_book is not None and ranges is not None

    # 2) Source language/version: do not infer from surface patterns.
    #    Defer to configured defaults and user/system preferences via resolver.
    requested_lang: Optional[str] = None

    # 3) Resolve bible data root path with fallbacks
    try:
        data_root, resolved_lang, resolved_version = resolve_bible_data_root(
            response_language=s.get("user_response_language"),
            query_language=s.get("query_language"),
            requested_lang=requested_lang,
            requested_version=None,
        )
        logger.info(
            "[retrieve-scripture] data_root=%s lang=%s version=%s",
            data_root,
            resolved_lang,
            resolved_version,
        )
    except FileNotFoundError:
        # Catalog available options for a friendly message
        avail = list_available_sources()
        if not avail:
            msg = (
                "Scripture data is not available on this server. Please contact the administrator."
            )
            return {"responses": [{"intent": IntentType.RETRIEVE_SCRIPTURE, "response": msg}]}
        options = ", ".join(f"{lang}/{ver}" for lang, ver in avail)
        msg = (
            f"I couldn't find a Bible source matching your request. Available sources: {options}. "
            f"Would you like me to use one of these?"
        )
        return {"responses": [{"intent": IntentType.RETRIEVE_SCRIPTURE, "response": msg}]}

    # 4) Retrieve exact verses
    verses = select_verses(data_root, canonical_book, ranges)
    if not verses:
        msg = (
            "I couldn't locate those verses in the Bible data. Please check the reference and try again."
        )
        return {"responses": [{"intent": IntentType.RETRIEVE_SCRIPTURE, "response": msg}]}

    # 5) Build header segments (book + suffix) and scripture segment
    ref_label = label_ranges(canonical_book, ranges)
    # Derive suffix by removing leading book name, when present
    suffix = ""
    if ref_label == canonical_book:
        suffix = ""
    elif ref_label.startswith(f"{canonical_book} "):
        suffix = ref_label[len(canonical_book) + 1 :]
    else:
        suffix = ref_label
    # Render as chapter:verse only to avoid repeating the book name from the header
    scripture_lines: list[str] = []
    for ref, txt in verses:
        parsed = parse_ch_verse_from_reference(ref)
        if parsed is None:
            scripture_lines.append(f"{txt}")
            continue
        ch, vs = parsed
        scripture_lines.append(f"{ch}:{vs} {txt}")
    scripture_text = "\n".join(scripture_lines)
    response_obj = {
        "suppress_translation": True,
        # Ensure downstream header language matches scripture language
        "content_language": str(resolved_lang),
        "header_is_translated": False,
        "segments": [
            {"type": "header_book", "text": canonical_book},
            {"type": "header_suffix", "text": suffix},
            {"type": "scripture", "text": scripture_text},
        ],
    }
    return {"responses": [{"intent": IntentType.RETRIEVE_SCRIPTURE, "response": response_obj}]}


def handle_translate_scripture(state: Any) -> dict:  # pylint: disable=too-many-branches,too-many-return-statements
    """Handle translate-scripture: return verses translated into a target language.

    - Extract passage selection via the shared helper.
    - Determine target language from the user's message; require it to be one of supported codes.
    - Optional source language/version parsing (simple language-name heuristic); if absent, use resolver fallbacks.
    - Load source verse texts, translate only the verse text per line, and return a structured, protected response.
    """
    s = cast(BrainState, state)
    query = s["transformed_query"]
    query_lang = s["query_language"]
    logger.info("[translate-scripture] start; query_lang=%s; query=%s", query_lang, query)

    # Parse passage selection
    canonical_book, ranges, err = _resolve_selection_for_single_book(query, query_lang)
    if err:
        return {"responses": [{"intent": IntentType.TRANSLATE_SCRIPTURE, "response": err}]}
    assert canonical_book is not None and ranges is not None

    # Enforce one-chapter cap: all ranges must resolve to exactly one chapter,
    # and all ranges must reference the same chapter.
    chapters: set[int] = set()
    multi_chapter = False
    for (sc, _sv, ec, _ev) in ranges:
        if sc is None:
            multi_chapter = True
            break
        ch_end = ec if ec is not None else sc
        if ch_end != sc:
            multi_chapter = True
            break
        chapters.add(sc)
    if multi_chapter or len(chapters) != 1:
        ref_label_over = label_ranges(canonical_book, ranges)
        msg = (
            "I can only translate one chapter at a time. "
            f"Your selection {ref_label_over} spans multiple chapters. "
            f"Please narrow the request to a single chapter (e.g., {canonical_book} 1)."
        )
        return {"responses": [{"intent": IntentType.TRANSLATE_SCRIPTURE, "response": msg}]}

    # Determine target language for translation
    # 1) Try to extract an explicit target from the message via structured parse
    target_language: Optional[str] = None
    try:
        tl_resp = open_ai_client.responses.parse(
            model="gpt-4o",
            instructions=TARGET_TRANSLATION_LANGUAGE_AGENT_SYSTEM_PROMPT,
            input=cast(Any, [{"role": "user", "content": f"message: {query}"}]),
            text_format=ResponseLanguage,
            temperature=0,
            store=False,
        )
        tl_usage = getattr(tl_resp, "usage", None)
        if tl_usage is not None:
            it = getattr(tl_usage, "input_tokens", None)
            ot = getattr(tl_usage, "output_tokens", None)
            tt = getattr(tl_usage, "total_tokens", None)
            if tt is None and (it is not None or ot is not None):
                tt = (it or 0) + (ot or 0)
            cit = _extract_cached_input_tokens(tl_usage)
            add_tokens(it, ot, tt, model="gpt-4o", cached_input_tokens=cit)
        tl_parsed = cast(ResponseLanguage | None, tl_resp.output_parsed)
        if tl_parsed and tl_parsed.language != Language.OTHER:
            target_language = str(tl_parsed.language.value)
    except OpenAIError:
        logger.info("[translate-scripture] target-language parse failed; will fallback", exc_info=True)

    # 2) Fallbacks: user_response_language, then detected query_language
    if not target_language:
        url = cast(Optional[str], s.get("user_response_language"))
        if url and url in supported_language_map:
            target_language = url
    if not target_language:
        ql = cast(Optional[str], s.get("query_language"))
        if ql and ql in supported_language_map:
            target_language = ql
    if not target_language:
        # Last resort: ask the user to set a response language
        supported = ", ".join(supported_language_map.keys())
        msg = (
            "I couldn't determine a target language for translation. "
            "Please specify a target (e.g., 'into Spanish') or set your response language. "
            f"Supported codes: {supported}."
        )
        return {"responses": [{"intent": IntentType.TRANSLATE_SCRIPTURE, "response": msg}]}

    # Optional explicit source language from query: avoid surface scans.
    requested_src_lang: Optional[str] = None

    # Resolve bible data root for source verses
    try:
        data_root, resolved_lang, resolved_version = resolve_bible_data_root(
            response_language=s.get("user_response_language"),
            query_language=s.get("query_language"),
            requested_lang=requested_src_lang,
            requested_version=None,
        )
        logger.info(
            "[translate-scripture] source data_root=%s lang=%s version=%s",
            data_root,
            resolved_lang,
            resolved_version,
        )
    except FileNotFoundError:
        avail = list_available_sources()
        if not avail:
            msg = (
                "Scripture data is not available on this server. Please contact the administrator."
            )
            return {"responses": [{"intent": IntentType.TRANSLATE_SCRIPTURE, "response": msg}]}
        options = ", ".join(f"{lang}/{ver}" for lang, ver in avail)
        msg = (
            f"I couldn't find a Bible source to translate from. Available sources: {options}. "
            f"Would you like me to use one of these?"
        )
        return {"responses": [{"intent": IntentType.TRANSLATE_SCRIPTURE, "response": msg}]}

    # Retrieve verses
    verses = select_verses(data_root, canonical_book, ranges)
    if not verses:
        msg = "I couldn't locate those verses in the Bible data. Please check the reference and try again."
        return {"responses": [{"intent": IntentType.TRANSLATE_SCRIPTURE, "response": msg}]}

    # Join body as continuous text (drop verse labels for single-call translation)
    body_src = "\n".join(txt for _, txt in verses)

    # Build header suffix once from the canonical label
    ref_label = label_ranges(canonical_book, ranges)
    if ref_label == canonical_book:
        header_suffix = ""
    elif ref_label.startswith(f"{canonical_book} "):
        header_suffix = ref_label[len(canonical_book) + 1 :]
    else:
        header_suffix = ref_label

    # Single parse call to translate book name + body, returning strict JSON
    messages: list[EasyInputMessageParam] = [
        {"role": "developer", "content": f"canonical_book: {canonical_book}"},
        {"role": "developer", "content": f"header_suffix (do not translate): {header_suffix}"},
        {"role": "developer", "content": f"target_language: {target_language}"},
        {"role": "developer", "content": "passage body (translate; preserve newlines):"},
        {"role": "developer", "content": body_src},
    ]
    translated: TranslatedPassage | None = None
    try:
        resp = open_ai_client.responses.parse(
            model="gpt-4o",
            instructions=TRANSLATE_PASSAGE_AGENT_SYSTEM_PROMPT,
            input=cast(Any, messages),
            text_format=TranslatedPassage,
            temperature=0,
            store=False,
        )
        usage = getattr(resp, "usage", None)
        if usage is not None:
            it = getattr(usage, "input_tokens", None)
            ot = getattr(usage, "output_tokens", None)
            tt = getattr(usage, "total_tokens", None)
            if tt is None and (it is not None or ot is not None):
                tt = (it or 0) + (ot or 0)
            cit = _extract_cached_input_tokens(usage)
            add_tokens(it, ot, tt, model="gpt-4o", cached_input_tokens=cit)
        translated = cast(TranslatedPassage | None, resp.output_parsed)
    except OpenAIError:
        logger.warning("[translate-scripture] structured parse failed due to OpenAI error; falling back.", exc_info=True)
        translated = None
    except (ValidationError, ValueError):
        logger.warning("[translate-scripture] structured parse failed; falling back to simple translation.", exc_info=True)
        translated = None

    # Build structured response (fallback to two-call approach if parse failed)
    response_obj: dict
    if translated is None:
        translated_body = translate_text(response_text=body_src, target_language=target_language)
        translated_book = translate_text(response_text=canonical_book, target_language=target_language)
        response_obj = {
            "suppress_translation": True,
            "content_language": target_language,
            "header_is_translated": True,
            "segments": [
                {"type": "header_book", "text": translated_book},
                {"type": "header_suffix", "text": header_suffix},
                {"type": "scripture", "text": translated_body},
            ],
        }
    else:
        response_obj = {
            "suppress_translation": True,
            "content_language": str(translated.content_language.value),
            "header_is_translated": True,
            "segments": [
                {"type": "header_book", "text": translated.header_book or canonical_book},
                {"type": "header_suffix", "text": translated.header_suffix or header_suffix},
                {"type": "scripture", "text": translated.body},
            ],
        }
    return {"responses": [{"intent": IntentType.TRANSLATE_SCRIPTURE, "response": response_obj}]}


def create_brain():
    """Assemble and compile the LangGraph for the BT Servant brain."""
    def wrap_node_with_timing(node_fn, node_name: str):  # type: ignore[no-untyped-def]
        def wrapped(state: Any) -> dict:
            trace_id = cast(dict, state).get("perf_trace_id")
            if trace_id:
                set_current_trace(cast(Optional[str], trace_id))
            with time_block(f"brain:{node_name}"):
                return node_fn(state)
        return wrapped
    def _make_state_graph(schema: Any) -> StateGraph[BrainState]:
        # Accept Any to satisfy IDE variance on schema param; schema is BrainState
        return StateGraph(schema)

    builder: StateGraph[BrainState] = _make_state_graph(BrainState)

    builder.add_node("start_node", wrap_node_with_timing(start, "start_node"))
    builder.add_node("determine_query_language_node", wrap_node_with_timing(determine_query_language, "determine_query_language_node"))
    builder.add_node("preprocess_user_query_node", wrap_node_with_timing(preprocess_user_query, "preprocess_user_query_node"))
    builder.add_node("determine_intents_node", wrap_node_with_timing(determine_intents, "determine_intents_node"))
    builder.add_node("set_response_language_node", wrap_node_with_timing(set_response_language, "set_response_language_node"))
    builder.add_node("query_vector_db_node", wrap_node_with_timing(query_vector_db, "query_vector_db_node"))
    builder.add_node("query_open_ai_node", wrap_node_with_timing(query_open_ai, "query_open_ai_node"))
    builder.add_node("chunk_message_node", wrap_node_with_timing(chunk_message, "chunk_message_node"))
    builder.add_node("handle_unsupported_function_node", wrap_node_with_timing(handle_unsupported_function, "handle_unsupported_function_node"))
    builder.add_node("handle_system_information_request_node", wrap_node_with_timing(handle_system_information_request, "handle_system_information_request_node"))
    builder.add_node("converse_with_bt_servant_node", wrap_node_with_timing(converse_with_bt_servant, "converse_with_bt_servant_node"))
    builder.add_node("handle_get_passage_summary_node", wrap_node_with_timing(handle_get_passage_summary, "handle_get_passage_summary_node"))
    builder.add_node("handle_get_passage_keywords_node", wrap_node_with_timing(handle_get_passage_keywords, "handle_get_passage_keywords_node"))
    builder.add_node("handle_get_translation_helps_node", wrap_node_with_timing(handle_get_translation_helps, "handle_get_translation_helps_node"))
    builder.add_node("handle_retrieve_scripture_node", wrap_node_with_timing(handle_retrieve_scripture, "handle_retrieve_scripture_node"))
    builder.add_node("handle_translate_scripture_node", wrap_node_with_timing(handle_translate_scripture, "handle_translate_scripture_node"))
    builder.add_node("translate_responses_node", wrap_node_with_timing(translate_responses, "translate_responses_node"), defer=True)

    builder.set_entry_point("start_node")
    builder.add_edge("start_node", "determine_query_language_node")
    builder.add_edge("determine_query_language_node", "preprocess_user_query_node")
    builder.add_edge("preprocess_user_query_node", "determine_intents_node")
    builder.add_conditional_edges(
        "determine_intents_node",
        process_intents
    )
    builder.add_edge("query_vector_db_node", "query_open_ai_node")
    builder.add_edge("set_response_language_node", "translate_responses_node")
    # After chunking, finish. Do not loop back to translate, which can recreate
    # the long message and trigger an infinite chunk cycle.

    builder.add_edge("handle_unsupported_function_node", "translate_responses_node")
    builder.add_edge("handle_system_information_request_node", "translate_responses_node")
    builder.add_edge("converse_with_bt_servant_node", "translate_responses_node")
    builder.add_edge("handle_get_passage_summary_node", "translate_responses_node")
    builder.add_edge("handle_get_passage_keywords_node", "translate_responses_node")
    builder.add_edge("handle_get_translation_helps_node", "translate_responses_node")
    builder.add_edge("handle_retrieve_scripture_node", "translate_responses_node")
    builder.add_edge("handle_translate_scripture_node", "translate_responses_node")
    builder.add_edge("query_open_ai_node", "translate_responses_node")

    builder.add_conditional_edges(
        "translate_responses_node",
        needs_chunking
    )
    builder.set_finish_point("chunk_message_node")

    return builder.compile()
